{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "lib_dir = \"/home/daniele/documents/github/ftt01/phd/share/lib\"\n",
    "sys.path.insert( 0, lib_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = \"/media/windows/projects/icon-evaluation/kriging/\"\n",
    "\n",
    "variables = ['precipitation', 'temperature', 'streamflow']\n",
    "\n",
    "basin = \"passirio\"\n",
    "\n",
    "start_date_str = '2010-01-01 00:00:00'\n",
    "end_date_str = '2021-10-17 00:00:00'\n",
    "timezone_str = 'Europe/Rome'\n",
    "\n",
    "start_date = dt.datetime.strptime(start_date_str, '%Y-%m-%d %H:%M:%S')\n",
    "end_date = dt.datetime.strptime(end_date_str, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "output_dt_format = '%Y-%m-%dT%H:%M:%S.%f%z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_weather_data_path = wdir + \"data/passirio/GS/\"\n",
    "\n",
    "### stations metadata\n",
    "metadata_data_path =  wdir + \"data/AA_weather_data/AA_stations_metadata.csv\"\n",
    "\n",
    "metadata = pd.read_csv( metadata_data_path )\n",
    "metadata = metadata[ metadata['ID_UI'] != '-999' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weather_data_path = \"/media/windows/data/meteo/eu/it/taa/aa/weather_stations/merged/\"\n",
    "input_dt_format = '%Y-%m-%dT%H:%M:%S.%f%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A. Creation of input for the kriging\n",
    "\n",
    "- for each variable to create the grid data set\n",
    "    - for each station available in the input directory\n",
    "        - select metadata of the station\n",
    "        - read the data of each available station data [ only the currently updated == the stations with ID_UI ]\n",
    "        - write the metadata and the data in the output directory in the format of kriging input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in variables:\n",
    "\n",
    "    st_list = []\n",
    "    for m in metadata.index:\n",
    "\n",
    "        internal_id = str(metadata.loc[m]['ID'])\n",
    "        # print(internal_id)\n",
    "\n",
    "        east = str(metadata.loc[m]['East'])\n",
    "        north = str(metadata.loc[m]['North'])\n",
    "        elevation = str(metadata.loc[m]['Elevation'])\n",
    "        id_ui = str(metadata.loc[m]['ID_UI'])\n",
    "\n",
    "        current_data_path = original_weather_data_path + \\\n",
    "            variable + \"/\" + str(internal_id) + \".csv\"\n",
    "        # print(current_data_path)\n",
    "\n",
    "        try:\n",
    "            current_data = read_timeseries_pd( pd.read_csv(current_data_path, header=0), input_dt_format=input_dt_format )\n",
    "        except FileNotFoundError as err:\n",
    "            print(\"FileNotFoundError: {0}\".format(err))\n",
    "            continue\n",
    "        except BaseException as err:\n",
    "            print(f\"Unexpected {err=}, {type(err)=}\")\n",
    "            raise\n",
    "\n",
    "        if variable == 'precipitation':\n",
    "            current_data = current_data.resample('h').sum()\n",
    "        elif variable == 'temperature':\n",
    "            current_data = current_data.resample('h').mean()\n",
    "        elif variable == 'streamflow':\n",
    "            current_data = current_data.resample('h').mean()\n",
    "        else:\n",
    "            print('NOT A VALID VARIABLE!')\n",
    "            continue\n",
    "            \n",
    "        current_data.index = [dt.datetime.strftime(\n",
    "            i, format=output_dt_format) for i in current_data.index]\n",
    "\n",
    "        df = current_data.to_csv(header=False).strip('\\n').split('\\n')\n",
    "        # <= this is the string that you can use with md5\n",
    "        data = '\\r\\n'.join(df)\n",
    "        # data = df_string.encode('utf8')  # <= this is bytes object to write the file\n",
    "        # print(df_bytes)\n",
    "\n",
    "        current_output_file = output_weather_data_path + \\\n",
    "            variable + \"/\" + internal_id + \".txt\"\n",
    "        mkNestedDir(getPathFromFilepath(current_output_file))\n",
    "\n",
    "        # ID,2.0\n",
    "        # x,642993.5\n",
    "        # y,5164882.0\n",
    "        # z,630.0\n",
    "        header = \"\"\"ID,{id}\\nx,{x}\\ny,{y}\\nz,{z}\\n\"\"\"\n",
    "\n",
    "        with open(current_output_file, 'w') as new:\n",
    "            new.write(header.format(id=internal_id,\n",
    "                      x=east, y=north, z=elevation))\n",
    "            new.write(data)\n",
    "\n",
    "        # add station to station list\n",
    "        st_props = {}\n",
    "\n",
    "        st_props[\"station_id\"] = internal_id\n",
    "        st_props[\"station_name\"] = str(metadata.loc[m]['Station']).lower().replace(\n",
    "            \" \", \"_\").replace(\"-\", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\")\n",
    "        st_props[\"east\"] = east\n",
    "        st_props[\"north\"] = north\n",
    "        st_props[\"elevation\"] = elevation\n",
    "\n",
    "        st_list.append(st_props)\n",
    "\n",
    "    with open(output_weather_data_path + variable + \".json\", \"w\") as data_file:\n",
    "        json.dump(st_list, data_file, indent=4, sort_keys=True)\n",
    "\n",
    "        # print(current_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creation of the *{variable}*.json metadata\n",
    "\n",
    "**NB:** BEFORE is necessary to create the file output from the extracted data and local DTM - the same of forecasting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_file = wdir + \"data/\" + basin + \"/output.csv\"\n",
    "grid_metadata = pd.read_csv( grid_file, index_col=1 )\n",
    "# grid_metadata.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "grid_metadata.dropna(inplace=True)\n",
    "\n",
    "st_list = []\n",
    "\n",
    "for id in grid_metadata.index:\n",
    "\n",
    "    st_props = {}\n",
    "\n",
    "    st_props[\"station_id\"] = str(id)\n",
    "    st_props[\"station_name\"] = str(id)\n",
    "    st_props[\"east\"] = str(grid_metadata.loc[id]['lat'])\n",
    "    st_props[\"north\"] = str(grid_metadata.loc[id]['lon'])\n",
    "    st_props[\"elevation\"] = str(grid_metadata.loc[id]['elevation'])\n",
    "\n",
    "    st_list.append( st_props )\n",
    "\n",
    "with open( wdir + \"data/\" + basin + \"/grid.json\", \"w\") as data_file:\n",
    "    json.dump(st_list, data_file, indent=4, sort_keys=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
