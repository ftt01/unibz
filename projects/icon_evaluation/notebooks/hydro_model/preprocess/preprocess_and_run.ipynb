{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "lib_dir = \"/home/daniele/documents/github/ftt01/phd/share/lib\"\n",
    "sys.path.insert( 0, lib_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stations(list_temperature, list_precipitation, list_streamflow, pathout):\n",
    "    mkNestedDir(pathout)\n",
    "\n",
    "    line_1 = \"{num_temperature}                 ! number of T stations\"\n",
    "    line_2 = \"{num_precipitation}               ! number of P stations\"\n",
    "    line_3 = \"{num_streamflow}                  ! number of Q stations\"\n",
    "    line_T = \"! T stations ID\"\n",
    "    line_P = \"! P stations ID\"\n",
    "    line_Q = \"! Q stations ID\"\n",
    "\n",
    "    line_1 = line_1.format(num_temperature=str(len(list_temperature)))\n",
    "    line_2 = line_2.format(num_precipitation=str(len(list_precipitation)))\n",
    "    line_3 = line_3.format(num_streamflow=str(len(list_streamflow)))\n",
    "\n",
    "    file = open(pathout+\"stations.txt\",\"w+\")\n",
    "    file.writelines(line_1 + \"\\n\")\n",
    "    file.writelines(line_2 + \"\\n\")\n",
    "    file.writelines(line_3 + \"\\n\")\n",
    "\n",
    "    file.writelines(line_T + \"\\n\")\n",
    "    for t in list_temperature:\n",
    "        file.writelines(str(t) + \"\\n\")\n",
    "\n",
    "    file.writelines(line_P + \"\\n\")\n",
    "    for p in list_precipitation:\n",
    "        file.writelines(str(p) + \"\\n\")\n",
    "\n",
    "    file.writelines(line_Q + \"\\n\")\n",
    "    for q in list_streamflow:\n",
    "        file.writelines(str(q) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_configuration_file(run_mode, start_wu_date, end_wu_date, start_forecast_date, end_forecast_date,\n",
    "                             parameters_file, basin, meteo_src, outlet_point, da_flag, wdir, pathout, kernel=True):\n",
    "    mkNestedDir(pathout)\n",
    "\n",
    "    if kernel != True:\n",
    "        wwdir = wdir\n",
    "        wdir = '../'\n",
    "    else:\n",
    "        wwdir = wdir\n",
    "\n",
    "    line_1 = \"{run_mode}                                ! run mode: 10 -> forecasting; 6 -> forward\".format(\n",
    "        run_mode=run_mode)\n",
    "    line_2 = \"1                                         ! ID run\"\n",
    "    line_3 = \"0 \t\t\t\t \t\t                ! ID previous run (useless)\"\n",
    "    line_4 = \"100 \t\t\t \t\t                    ! ID run meteorological fields (useless)\"\n",
    "    line_5 = \"0  \t \t\t\t \t\t                ! T forecasts correction: 1 -> yes; 0 -> no (useless)\"\n",
    "    line_6 = \"0 \t\t\t\t \t\t                ! T forecasts correction: 1 -> yes; 0 -> no (useless)\"\n",
    "\n",
    "    # warmup period\n",
    "    line_9 = \"{start_wu_date} \t\t                    ! start warm - up\".format(\n",
    "        start_wu_date=dt.datetime.strftime(start_wu_date, format='%Y-%m-%d_%H:%M'))\n",
    "    line_10 = \"{end_wu_date} \t\t                    ! begin D.A. (end warm-up)\".format(\n",
    "        end_wu_date=dt.datetime.strftime(end_wu_date, format='%Y-%m-%d_%H:%M'))\n",
    "\n",
    "    # DA starts one hour later than line_10 and it ends at line_7\n",
    "\n",
    "    # forecast period\n",
    "    line_7 = \"{start_forecast_date}                     ! end D.A. (start forecast/forward)\".format(\n",
    "        start_forecast_date=dt.datetime.strftime(start_forecast_date, format='%Y-%m-%d_%H:%M'))\n",
    "    line_8 = \"{end_forecast_date} \t                    ! end forecast/forward\".format(\n",
    "        end_forecast_date=dt.datetime.strftime(end_forecast_date, format='%Y-%m-%d_%H:%M'))\n",
    "\n",
    "    # parameters to run the model\n",
    "    line_11 = \"\\\"{parameters_file}\\\" \t                    ! parameter file name\".format(\n",
    "        parameters_file=parameters_file)\n",
    "\n",
    "    line_12 = \"\\\"subcatchments.asc\\\"                     ! subcatchment raster file name\"\n",
    "    line_13 = \"\\\"{wdir}{basin}/INPUT/\\\" \t                ! path of required input data\".format(\n",
    "        wdir=wdir, basin=basin)\n",
    "    \n",
    "    line_14 = \"\\\"{wdir}{basin}/OUTPUT/{date}/\\\" \t         ! general folder of output data\"\n",
    "    if run_mode == '10':\n",
    "        line_14 = line_14.format(wdir=wdir, basin=basin, date=dt.datetime.strftime(start_forecast_date, format='%Y%m%d'))\n",
    "        out_path = \"{wdir}{basin}/OUTPUT/{date}/\".format(wdir=wwdir, basin=basin, date=dt.datetime.strftime(start_forecast_date, format='%Y%m%d'))\n",
    "    else:\n",
    "        line_14 = line_14.format(wdir=wdir, basin=basin, date='')\n",
    "        out_path = \"{wdir}{basin}/OUTPUT/\".format(wdir=wwdir, basin=basin)\n",
    "\n",
    "    mkNestedDir(out_path)\n",
    "\n",
    "    line_15 = \"\\\"{wdir}{basin}/meteo/\\\"                     ! path of required meteorological data (obs. and for.)\".format(\n",
    "        wdir=wdir, basin=basin)\n",
    "    line_16 = \"\\\"{meteo_src}\\\"\t\t\t                ! observed meteo source\".format(\n",
    "        meteo_src=meteo_src)\n",
    "    line_17 = \"30.0\t\t\t\t\t                    ! cellsize of subcatchment map\"\n",
    "    line_18 = \"5.0\t\t\t\t\t\t                ! mm SWE for SCA threshold\"\n",
    "    line_19 = \"46.7 \t\t\t\t\t                    ! mean latitude\"\n",
    "    line_20 = \"2.0\t\t\t\t\t\t                ! minimum number of meteorological station to perform interpolation\"\n",
    "    line_21 = \"\\\"NASH\\\" \t\t\t\t\t                ! objective function for model calibration\"\n",
    "    line_22 = \"1 \t\t\t\t\t\t                ! number of calibration points\"\n",
    "    line_23 = \"{outlet_point} \t\t\t\t\t        ! ID calibration point\".format(\n",
    "        outlet_point=outlet_point)\n",
    "    line_24 = \"{outlet_point} \t\t\t\t\t        ! ID outlet section\".format(\n",
    "        outlet_point=outlet_point)\n",
    "    line_25 = \"1 \t\t\t\t\t\t                ! model type for base\"\n",
    "    line_26 = \"{da_flag} \t\t\t\t\t\t        ! flag Data Assimilation: 1 -> active; 0 -> open-loop\".format(\n",
    "        da_flag=da_flag)\n",
    "\n",
    "    file = open(pathout+\"configuration_file.txt\", \"w+\")\n",
    "    [file.writelines(l + \"\\n\") for l in [line_1, line_2, line_3, line_4, line_5, line_6, line_7, line_8, line_9, line_10, line_11,\n",
    "                                         line_12, line_13, line_14, line_15, line_16, line_17, line_18, line_19, line_20, line_21,\n",
    "                                         line_22, line_23, line_24, line_25, line_26]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_DA_settings(ensemble_size, pathout):\n",
    "    mkNestedDir(pathout)\n",
    "\n",
    "    line_1 = \"{ensemble_size} \t\t\t! ensemble size\".format(ensemble_size=ensemble_size)\n",
    "    line_2 = \"5 \t\t\t            ! number of state variables\"\n",
    "    line_3 = \"R C S G B \t            ! labels of state variables\"\n",
    "    line_4 = \"0.00\t\t                ! percentage error of observed Q \"\n",
    "    line_5 = \"0.01\t\t\t            ! percentage error for further perturbing observed streamflow\"\n",
    "    line_6 = \"0.02\t\t                ! threshold on precipitation perturbation\"\n",
    "    line_7 = \"1.0\t\t\t            ! shape gamma pdf for P perturbation (k or alpha)      --> USELESS\"\n",
    "    line_8 = \"1.0\t\t\t            ! scale gamma pdf for P perturbation (beta or 1/theta) --> USELESS\"\n",
    "    line_9 = \"0. \t\t\t            ! dev. std. T perturbation\"\n",
    "    line_10 = \"0\t\t\t            ! additive inflation on soil water content (0: no; 1: yes)\"\n",
    "    line_11 = \"0.00\t\t                ! random noise for additive inflation\"\n",
    "    line_12 = \"1\t    \t            ! time step between every analysis step\"\n",
    "\n",
    "    file = open( pathout + \"DA_settings.txt\", \"w+\")\n",
    "    [file.writelines(l + \"\\n\") for l in [line_1, line_2, line_3, line_4, line_5, line_6,\n",
    "                                         line_7, line_8, line_9, line_10, line_11, line_12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = \"/media/windows/projects/icon-evaluation/\"\n",
    "\n",
    "variables = ['precipitation', 'temperature']\n",
    "basin = \"passirio\"\n",
    "\n",
    "timezone_str = 'Europe/Rome'\n",
    "\n",
    "# kriging dates - NB: must be the same of the kriging simulation\n",
    "start_kr_date_str = '2010-10-01 00:00:00'\n",
    "end_kr_date_str = '2021-10-17 00:00:00'\n",
    "\n",
    "start_kr_date = dt.datetime.strptime(\n",
    "    start_kr_date_str, '%Y-%m-%d %H:%M:%S').replace(tzinfo=ZoneInfo(timezone_str))\n",
    "end_kr_date = dt.datetime.strptime(\n",
    "    end_kr_date_str, '%Y-%m-%d %H:%M:%S').replace(tzinfo=ZoneInfo(timezone_str))\n",
    "\n",
    "dates = [start_kr_date + dt.timedelta(hours=x)\n",
    "         for x in range(1, (end_kr_date-start_kr_date).days*24 + 1)]\n",
    "\n",
    "# forecasting dates\n",
    "start_fct_date_str = '2021-06-16 00:00:00'\n",
    "end_fct_date_str = '2021-10-17 00:00:00'\n",
    "\n",
    "start_fct_date = dt.datetime.strptime(\n",
    "    start_fct_date_str, '%Y-%m-%d %H:%M:%S').replace(tzinfo=ZoneInfo(timezone_str))\n",
    "end_fct_date = dt.datetime.strptime(\n",
    "    end_fct_date_str, '%Y-%m-%d %H:%M:%S').replace(tzinfo=ZoneInfo(timezone_str))\n",
    "\n",
    "fct_dates = [start_fct_date + dt.timedelta(days=x)\n",
    "             for x in range((end_fct_date-start_fct_date).days)]\n",
    "\n",
    "output_dt_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "# setup simulations\n",
    "run_mode = \"10\"\n",
    "\n",
    "calibration_lag_hours = 1*300*24\n",
    "wu_days = 360\n",
    "da_days = 10\n",
    "forecasting_lead_time = 24\n",
    "\n",
    "da_flag = \"1\"\n",
    "ensemble_size = 20\n",
    "\n",
    "parameters_file = \"hydro_parameters_GLUE_216.txt\"\n",
    "meteo_src = \"GS\"\n",
    "outlet_points = [\"118\", \"155\"]\n",
    "\n",
    "# ### ICHYMOD LINUX\n",
    "# linux = True\n",
    "# exe_path = \"/home/daniele/documents/github/ftt01/phd/models/hydro_modeling/ichymod/core/release/\"\n",
    "# exe_name = \"ichymod\"\n",
    "\n",
    "# ICHYMOD WINZOZ\n",
    "linux = False\n",
    "exe_path = \"/home/daniele/documents/github/ftt01/phd/models/hydro_modeling/ichymod/core/VS_project/x64/Debug/\"\n",
    "exe_name = \"ichymod.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### stations metadata\n",
    "## the grid is obtained in QGIS from the output.csv and the DTM elevation\n",
    "metadata_data_path =  wdir + \"data/GIS/grid.json\"\n",
    "\n",
    "grid_file = open(metadata_data_path)\n",
    "grid_meta = json.load(grid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_temperature = []\n",
    "list_precipitation = []\n",
    "\n",
    "# for each element in the forecasting dates\n",
    "#   - setup the dates to use the right windows of data\n",
    "#   for each variable in variables\n",
    "#       for each point of the grid\n",
    "break_flag = False\n",
    "\n",
    "for fct_date in fct_dates:\n",
    "\n",
    "    current_start_fct_date = fct_date\n",
    "    current_end_fct_date = current_start_fct_date + \\\n",
    "        dt.timedelta(hours=forecasting_lead_time)\n",
    "\n",
    "    start_date = current_start_fct_date - \\\n",
    "        dt.timedelta(hours=calibration_lag_hours)\n",
    "    end_date = current_end_fct_date\n",
    "\n",
    "    start_date_str = dt.datetime.strftime( start_date, '%Y-%m-%d %H:%M:%S' )\n",
    "    end_date_str = dt.datetime.strftime( end_date, '%Y-%m-%d %H:%M:%S' )\n",
    "\n",
    "    start_wu_date = start_date - dt.timedelta(days=wu_days+da_days)\n",
    "    end_wu_date = current_start_fct_date - dt.timedelta(days=da_days)\n",
    "\n",
    "    if os.path.exists(wdir + \"hydro_modeling/\" +\n",
    "                      basin + \"/meteo/forecast/{date}/\".format(\n",
    "                          date=dt.datetime.strftime(fct_date, '%Y%m%d'))) == False:\n",
    "        print(\"Forecast data missing: \" + fct_weather_data_path)\n",
    "        continue\n",
    "\n",
    "    for variable in variables:\n",
    "\n",
    "        print(variable)\n",
    "\n",
    "        # input of forecasting data\n",
    "        fct_weather_data_path = wdir + \"hydro_modeling/\" + \\\n",
    "            basin + \"/meteo/forecast/{date}/{variable}/\".format(\n",
    "                variable=variable, date=dt.datetime.strftime(fct_date, '%Y%m%d'))\n",
    "        output_weather_fct_path = wdir + \"hydro_modeling/\" + \\\n",
    "            basin + \"/meteo/GS/forecasts/{variable}/\".format(\n",
    "                variable=variable)\n",
    "\n",
    "        # simply copy the two directories from `extract_forecast.ipynb`\n",
    "        copy_content(fct_weather_data_path, output_weather_fct_path)\n",
    "\n",
    "        if variable == 'temperature':\n",
    "            kr_corr = 'NO'\n",
    "        elif variable == 'precipitation':\n",
    "            kr_corr = 'DEUTSCH'\n",
    "\n",
    "        st_list = []\n",
    "        for g in grid_meta:\n",
    "\n",
    "            output_weather_data_path = wdir + \"hydro_modeling/\" + \\\n",
    "                basin + \"/meteo/GS/observations/{variable}/\"\n",
    "\n",
    "            grid_weather_data_path = wdir + \\\n",
    "                \"kriging/output_passirio_stations_complete/{variable}/KR/{kr_type}/{kr_correction}/\"\n",
    "\n",
    "            internal_id = str(g['station_id'])\n",
    "            east = str(g['east'])\n",
    "            north = str(g['north'])\n",
    "            elevation = str(g['elevation'])\n",
    "\n",
    "            current_data_path = grid_weather_data_path.format(\n",
    "                variable=variable, kr_type='OKED', kr_correction=kr_corr)\n",
    "            # print(current_data_path)\n",
    "\n",
    "            current_file = glob.glob(\n",
    "                current_data_path + '*_{id}_*.csv'.format(id=internal_id))[0]\n",
    "            current_data = pd.DataFrame(pd.read_csv(current_file, header=0)[\n",
    "                                        internal_id].values, index=dates, columns=['values'])\n",
    "            current_data.index.name = 'datetime'\n",
    "\n",
    "            current_data = current_data[start_date:end_date]\n",
    "\n",
    "            if variable == 'precipitation':\n",
    "                current_data = current_data.resample('h').sum()\n",
    "            elif variable == 'temperature':\n",
    "                current_data = current_data.resample('h').mean()\n",
    "            else:\n",
    "                print('NOT A VALID VARIABLE!')\n",
    "                continue\n",
    "\n",
    "            current_data.index = [dt.datetime.strftime(\n",
    "                i, format=output_dt_format) for i in current_data.index]\n",
    "            df = current_data.to_csv(header=False).strip('\\n').split('\\n')\n",
    "            data = '\\r\\n'.join(df)\n",
    "\n",
    "            current_output_file = output_weather_data_path.format(\n",
    "                variable=variable) + internal_id + \".txt\"\n",
    "            mkNestedDir(getPathFromFilepath(current_output_file))\n",
    "\n",
    "            # ID,2.0\n",
    "            # x,642993.5\n",
    "            # y,5164882.0\n",
    "            # z,630.0\n",
    "            header = \"\"\"ID,{id}\\nx,{x}\\ny,{y}\\nz,{z}\\n\"\"\"\n",
    "\n",
    "            with open(current_output_file, 'w') as new:\n",
    "                new.write(header.format(id=internal_id,\n",
    "                                        x=east, y=north, z=elevation))\n",
    "                new.write(data)\n",
    "\n",
    "            # add station to station list\n",
    "            if variable == 'precipitation':\n",
    "                list_precipitation.append(internal_id)\n",
    "            elif variable == 'temperature':\n",
    "                list_temperature.append(internal_id)\n",
    "            else:\n",
    "                print('Not a valid variable!')\n",
    "                continue\n",
    "\n",
    "            ########################################################\n",
    "            # FORECAST DATA - extraction on the forecasting period\n",
    "            # output_weather_fct_path\n",
    "            in_fct_file = output_weather_fct_path + '{id}.csv'.format(\n",
    "                id=internal_id)\n",
    "            out_fct_file = output_weather_fct_path + '{id}.txt'.format(\n",
    "                id=internal_id)\n",
    "\n",
    "            try:\n",
    "                current_fct_data = read_timeseries_pd(pd.read_csv(in_fct_file, header=None, skiprows=4),\n",
    "                                                      input_dt_format=\"%Y-%m-%d %H:%M:%S\", datetime_col=0)\n",
    "            except FileNotFoundError as err:\n",
    "                print(err)\n",
    "                continue\n",
    "            except:\n",
    "                raise\n",
    "\n",
    "            current_fct_data.index.name = 'datetime'\n",
    "            os.remove(in_fct_file)\n",
    "\n",
    "            current_fct_data = current_fct_data[start_fct_date_str:end_fct_date_str]\n",
    "\n",
    "            if variable == 'precipitation':\n",
    "                current_fct_data = current_fct_data.resample('h').sum()\n",
    "            elif variable == 'temperature':\n",
    "                current_fct_data = current_fct_data.resample('h').mean()\n",
    "            else:\n",
    "                print('NOT A VALID VARIABLE!')\n",
    "                continue\n",
    "\n",
    "            current_fct_data.index = [dt.datetime.strftime(\n",
    "                i, format=output_dt_format) for i in current_fct_data.index]\n",
    "            df = current_fct_data.to_csv(header=False).strip('\\n').split('\\n')\n",
    "            data = '\\r\\n'.join(df)\n",
    "\n",
    "            # ID,2.0\n",
    "            # x,642993.5\n",
    "            # y,5164882.0\n",
    "            # z,630.0\n",
    "            header = \"\"\"ID,{id}\\nx,{x}\\ny,{y}\\nz,{z}\\n\"\"\"\n",
    "\n",
    "            with open(out_fct_file, 'w') as new:\n",
    "                new.write(header.format(id=internal_id,\n",
    "                                        x=east, y=north, z=elevation))\n",
    "                new.write(data)\n",
    "\n",
    "            # add station to station list\n",
    "            if variable == 'precipitation':\n",
    "                list_precipitation.append(internal_id)\n",
    "            elif variable == 'temperature':\n",
    "                list_temperature.append(internal_id)\n",
    "            else:\n",
    "                print('Not a valid variable!')\n",
    "                continue\n",
    "\n",
    "    streamflow_data_path = wdir + \"kriging/data/\" + basin + \"/GS/streamflow/\"\n",
    "    output_streamflow_data_path = wdir + \\\n",
    "        \"hydro_modeling/\" + basin + \"/meteo/streamflow/\"\n",
    "\n",
    "    streamflow_files = glob.glob(streamflow_data_path + '*.txt')\n",
    "\n",
    "    for el in streamflow_files:\n",
    "        id = str(os.path.basename(el[:-4]))\n",
    "        if id in outlet_points:\n",
    "\n",
    "            # collect matadata\n",
    "            current_metadata = pd.read_csv(el, header=0)[0:4]\n",
    "            x = current_metadata[id][0]\n",
    "            y = current_metadata[id][1]\n",
    "            z = current_metadata[id][2]\n",
    "            area = current_metadata[id][3]\n",
    "\n",
    "            # collect streamflow data\n",
    "            current_data = pd.read_csv(el, skiprows=5, index_col=0, parse_dates=True, header=None,\n",
    "                                       names=['datetime', 'values'])\n",
    "\n",
    "            current_data = current_data[start_date:end_date]\n",
    "            current_data['values'] = current_data['values'].replace(\n",
    "                np.nan, -999.0)\n",
    "\n",
    "            current_data.index = [dt.datetime.strftime(\n",
    "                i, format=output_dt_format) for i in current_data.index]\n",
    "\n",
    "            df = current_data.to_csv(header=False).strip('\\n').split('\\n')\n",
    "            data = '\\r\\n'.join(df)\n",
    "\n",
    "            # ID,2.0\n",
    "            # x,642993.5\n",
    "            # y,5164882.0\n",
    "            # z,630.0\n",
    "            # area,-999.0\n",
    "            header = \"\"\"ID,{id}\\nx,{x}\\ny,{y}\\nz,{z}\\narea,{area}\\n\"\"\"\n",
    "\n",
    "            mkNestedDir(output_streamflow_data_path)\n",
    "            with open(output_streamflow_data_path + os.path.basename(el[:-4]) + '.txt', 'w') as new:\n",
    "                new.write(header.format(id=id,\n",
    "                                        x=x, y=y, z=z, area=area))\n",
    "                new.write(data)\n",
    "\n",
    "    write_stations(list_temperature, list_precipitation,\n",
    "                   outlet_points, wdir + \"hydro_modeling/\" + basin + \"/INPUT/\")\n",
    "\n",
    "    ##################################################################\n",
    "\n",
    "    input_weather_data_path = wdir + \"hydro_modeling/\" + \\\n",
    "        basin + \"/meteo/GS/observations/\"\n",
    "    output_weather_data_path = wdir + \"hydro_modeling/\" + basin + \"/meteo/GS/forecasts/\"\n",
    "\n",
    "    # grid_weather_data_path = wdir + \"kriging/output_passirio_stations_complete/{variable}/KR/{kr_type}/{kr_correction}/\"\n",
    "\n",
    "    # stations metadata\n",
    "    # the grid is obtained in QGIS from the output.csv and the DTM elevation\n",
    "    metadata_data_path = wdir + \"data/GIS/grid.json\"\n",
    "\n",
    "    grid_file = open(metadata_data_path)\n",
    "    grid_meta = json.load(grid_file)\n",
    "\n",
    "    list_temperature = []\n",
    "    list_precipitation = []\n",
    "\n",
    "    for variable in variables:\n",
    "\n",
    "        print(variable)\n",
    "\n",
    "        st_list = []\n",
    "        for g in grid_meta:\n",
    "\n",
    "            internal_id = str(g['station_id'])\n",
    "            east = str(g['east'])\n",
    "            north = str(g['north'])\n",
    "            elevation = str(g['elevation'])\n",
    "\n",
    "            # current_data_path = grid_weather_data_path.format(variable=variable, kr_type='OKED', kr_correction=kr_corr)\n",
    "            # # print(current_data_path)\n",
    "            current_input_weather_data_path = input_weather_data_path + variable + \"/\"\n",
    "\n",
    "            current_file = glob.glob(\n",
    "                current_input_weather_data_path + '{id}.txt'.format(id=internal_id))[0]\n",
    "            current_data = pd.read_csv(current_file, header=None, skiprows=4,\n",
    "                                       parse_dates=True, index_col=0, names=['datetime', 'values'])\n",
    "            try:\n",
    "                current_data = current_data.tz_localize(\n",
    "                    timezone_str, ambiguous='infer', nonexistent='NaT')\n",
    "            except:\n",
    "                current_data = current_data.tz_localize(\n",
    "                    timezone_str, ambiguous=True, nonexistent='NaT')\n",
    "\n",
    "            current_data = current_data[current_start_fct_date:current_end_fct_date]\n",
    "\n",
    "            if variable == 'precipitation':\n",
    "                current_data = current_data.resample('h').sum()\n",
    "            elif variable == 'temperature':\n",
    "                current_data = current_data.resample('h').mean()\n",
    "            else:\n",
    "                print('NOT A VALID VARIABLE!')\n",
    "                continue\n",
    "\n",
    "            current_data.index = [dt.datetime.strftime(\n",
    "                i, format=output_dt_format) for i in current_data.index]\n",
    "            df = current_data.to_csv(header=False).strip('\\n').split('\\n')\n",
    "            data = '\\r\\n'.join(df)\n",
    "\n",
    "            current_output_file = output_weather_data_path + \\\n",
    "                variable + \"/\" + internal_id + \".txt\"\n",
    "            mkNestedDir(getPathFromFilepath(current_output_file))\n",
    "\n",
    "            # ID,2.0\n",
    "            # x,642993.5\n",
    "            # y,5164882.0\n",
    "            # z,630.0\n",
    "            header = \"\"\"ID,{id}\\nx,{x}\\ny,{y}\\nz,{z}\\n\"\"\"\n",
    "\n",
    "            with open(current_output_file, 'w') as new:\n",
    "                new.write(header.format(id=internal_id,\n",
    "                                        x=east, y=north, z=elevation))\n",
    "                new.write(data)\n",
    "\n",
    "            # add station to station list\n",
    "            if variable == 'precipitation':\n",
    "                list_precipitation.append(internal_id)\n",
    "            elif variable == 'temperature':\n",
    "                list_temperature.append(internal_id)\n",
    "            else:\n",
    "                print('NOT A VALID VARIABLE!')\n",
    "                continue\n",
    "\n",
    "    ##################################################################\n",
    "\n",
    "    write_configuration_file(run_mode, start_wu_date, end_wu_date, current_start_fct_date, current_end_fct_date,\n",
    "                             parameters_file, basin, meteo_src, outlet_points[\n",
    "                                 0], da_flag, wdir + \"hydro_modeling/\",\n",
    "                             wdir + \"hydro_modeling/\" + basin + \"/INPUT/\", kernel=linux)\n",
    "\n",
    "    write_DA_settings(ensemble_size, wdir +\n",
    "                      \"hydro_modeling/\" + basin + \"/INPUT/\")\n",
    "\n",
    "    # RUN THE MODEL\n",
    "    os.chdir(wdir + \"hydro_modeling/\" + basin + \"/\")\n",
    "    p = subprocess.Popen(exe_path + exe_name,\n",
    "                         stdin=subprocess.PIPE, shell=True)\n",
    "    p.communicate(input='\\n'.encode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamflow_data_path = wdir + \"kriging/data/\" + basin + \"/GS/streamflow/\"\n",
    "# output_streamflow_data_path = wdir + \"hydro_modeling/\" + basin + \"/meteo/streamflowDA/\"\n",
    "\n",
    "# list_streamflow = ['118','155']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamflow_files = glob.glob(streamflow_data_path + '*.txt')\n",
    "\n",
    "# for el in streamflow_files:\n",
    "#     id = str(os.path.basename(el[:-4]))\n",
    "#     if id in list_streamflow:\n",
    "\n",
    "#         ### collect matadata\n",
    "#         current_metadata = pd.read_csv(el, header=0)[0:4]\n",
    "#         x = current_metadata[id][0]\n",
    "#         y = current_metadata[id][1]\n",
    "#         z = current_metadata[id][2]\n",
    "#         area = current_metadata[id][3]\n",
    "\n",
    "#         ### collect streamflow data\n",
    "#         current_data = pd.read_csv(el, skiprows=5, index_col=0, parse_dates=True, header=None,\n",
    "#                                 names=['datetime', 'values'])\n",
    "\n",
    "#         current_data = current_data[start_date:end_date]\n",
    "#         current_data['values'] = current_data['values'].replace(np.nan, -999.0)\n",
    "\n",
    "#         current_data.index = [dt.datetime.strftime(\n",
    "#             i, format=output_dt_format) for i in current_data.index]\n",
    "        \n",
    "#         df = current_data.to_csv(header=False).strip('\\n').split('\\n')\n",
    "#         data = '\\r\\n'.join(df)\n",
    "\n",
    "#         # ID,2.0\n",
    "#         # x,642993.5\n",
    "#         # y,5164882.0\n",
    "#         # z,630.0\n",
    "#         # area,-999.0\n",
    "#         header = \"\"\"ID,{id}\\nx,{x}\\ny,{y}\\nz,{z}\\narea,{area}\\n\"\"\"\n",
    "\n",
    "#         mkNestedDir(output_streamflow_data_path)\n",
    "#         with open(output_streamflow_data_path + os.path.basename(el[:-4]) + '.txt', 'w') as new:\n",
    "#             new.write(header.format(id=id,\n",
    "#                         x=x, y=y, z=z, area=area))\n",
    "#             new.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
