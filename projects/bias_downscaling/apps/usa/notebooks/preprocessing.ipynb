{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "lib_dir = \"/home/daniele/documents/github/ftt01/phd/share/lib\"\n",
    "sys.path.insert( 0, lib_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = \"/media/windows/projects/bias_correction/data/meteo/cmcc_cm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_first_row(file_path_name):\n",
    "    out_path_name = file_path_name[:-4] + \"_temporary_file.csv\"\n",
    "    # print(out_path_name)\n",
    "    # command = '''sed -i '1d' {file_path_name}'''\n",
    "    command = '''tail -n +2 {file_path_name} > {out_path_name}'''\n",
    "    p = subprocess.Popen(command.format(file_path_name=file_path_name, out_path_name=out_path_name),\n",
    "                         shell=True, stdout=subprocess.PIPE)\n",
    "    \n",
    "    return out_path_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_outputs = []\n",
    "output_path = \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/\"\n",
    "mkNestedDir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMCC data\n",
    "input_paths = [\n",
    "    wdir + \"precipitation_model/pr_hist_entire_period/output/\",\n",
    "    wdir + \"precipitation_model/pr_project/output/\",\n",
    "    wdir + \"temperature_model/tas_hist_entire_period/output/\",\n",
    "    wdir + \"temperature_model/tas_project/output/\"\n",
    "]\n",
    "\n",
    "for input_path in input_paths:\n",
    "\n",
    "    station_dirs = glob.glob(input_path + \"*/\")\n",
    "\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for station_dir in station_dirs:\n",
    "        # print(\"Opening: \" + station_dir)\n",
    "        to_aggregate_files = glob.glob(station_dir + \"*.csv\")\n",
    "\n",
    "        station_id = station_dir.split(\"/\")[-2]\n",
    "        # print(\"station_id: \" + station_id)\n",
    "        var = station_dir.split(\"/\")[-4].split(\"_\")[0]\n",
    "        # period = station_dir.split(\"/\")[-4].split(\"_\")[1]\n",
    "\n",
    "        tmp_aggr_data = pd.DataFrame()\n",
    "\n",
    "        for f in to_aggregate_files:\n",
    "\n",
    "            if \"_temporary_file\" in f:\n",
    "                try:\n",
    "                    os.remove(f)\n",
    "                    print(\"Removed: \" + f)\n",
    "                    continue\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            print(f)\n",
    "\n",
    "            f = delete_first_row(f)\n",
    "            time.sleep(3)\n",
    "            tmp_data = pd.read_csv(f, skiprows=0, header=None)\n",
    "            tmp_data['date'] = pd.to_datetime(tmp_data[0], format=\"%m/%Y\")\n",
    "            tmp_data.drop(columns=[0], inplace=True)\n",
    "            tmp_data.set_index('date', inplace=True)\n",
    "            os.remove(f)\n",
    "\n",
    "            # tmp_data = tmp_data.rename(columns={1:\"values\"})\n",
    "\n",
    "            tmp_aggr_data = pd.concat([tmp_aggr_data, tmp_data]).sort_index()\n",
    "\n",
    "        # tmp_aggr_data.index = [pd.to_datetime(dt.datetime.strftime(t, \"%Y-%m-%d %H:%M:%S\"), format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        #                        for t in tmp_aggr_data.index]\n",
    "\n",
    "        try:\n",
    "            all_data[station_id] = tmp_aggr_data\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        del [station_id]\n",
    "\n",
    "    all_data.index = [dt.datetime.strftime(\n",
    "        t, \"%Y-%m-%d %H:%M:%S\") for t in all_data.index]\n",
    "    all_data.index.name = 'datetime'\n",
    "    # all_data.to_csv( input_path + \"{var}_{period}.csv\".format(var=var, period=period) )\n",
    "    output_path_name = output_path + station_dir.split(\"/\")[-4] + \".csv\"\n",
    "    all_data.to_csv( output_path_name )\n",
    "    list_of_outputs.append( output_path_name )\n",
    "    del [output_path_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### observed data\n",
    "input_path = wdir + \"observations/obs/\"\n",
    "\n",
    "pr_data = pd.DataFrame()\n",
    "tas_data = pd.DataFrame()\n",
    "\n",
    "station_files = glob.glob( input_path + \"*.xlsx\" )\n",
    "\n",
    "for station_file in station_files:\n",
    "\n",
    "    tmp_file = pd.read_excel(station_file,engine='openpyxl')\n",
    "    \n",
    "    try:\n",
    "        tmp_file = pd.read_excel(station_file,engine='openpyxl')\n",
    "    except:\n",
    "        print(\"Skipping..\" + e)\n",
    "        continue\n",
    "\n",
    "    tmp_data = tmp_file[['PRCP (mm)','TAVG (°C)']]\n",
    "    tmp_data.index = [ pd.to_datetime(d, format=\"%Y-%m\") for d in tmp_file['DATE'] ]\n",
    "\n",
    "    del [tmp_file]\n",
    "\n",
    "    station_id = station_file.split(\"/\")[-1].split(\"_\")[0]\n",
    "    \n",
    "    if station_id == 'TX':\n",
    "        station_id = station_file.split(\"/\")[-1].split(\"_\")[0] + '_' + station_file.split(\"/\")[-1].split(\"_\")[1]\n",
    "    # print(\"station_id: \" + station_id)\n",
    "\n",
    "    pr_data[station_id] = tmp_data[['PRCP (mm)']]\n",
    "    tas_data[station_id] = tmp_data[['TAVG (°C)']]\n",
    "    \n",
    "pr_data.index = [dt.datetime.strftime(t, \"%Y-%m-%d %H:%M:%S\") for t in pr_data.index]\n",
    "pr_data.index.name = 'datetime'\n",
    "output_path_name = output_path + \"{var}_{period}.csv\".format(var='pr', period='obs')\n",
    "pr_data.to_csv( output_path_name )\n",
    "list_of_outputs.append( output_path_name )\n",
    "del [output_path_name]\n",
    "\n",
    "tas_data.index = [dt.datetime.strftime(t, \"%Y-%m-%d %H:%M:%S\") for t in tas_data.index]\n",
    "tas_data.index.name = 'datetime'\n",
    "output_path_name = output_path + \"{var}_{period}.csv\".format(var='tas', period='obs')\n",
    "tas_data.to_csv( output_path_name )\n",
    "list_of_outputs.append( output_path_name )\n",
    "del [output_path_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HISTORICAL CASE STUDY\n",
    "# 1970-1995 calibration + 1996-2005 validation == 1970-2005 timeseries\n",
    "\n",
    "metadata_01 = {\n",
    "    \"testcase\": \"testcase_01\",\n",
    "    \"variables\": [{\n",
    "        \"variable\": \"precipitation\",\n",
    "        \"calibration\": {\"start_datetime\": \"1970/01/01 00:00:00\",\n",
    "                        \"end_datetime\": \"1995/12/31 23:00:00\",\n",
    "                        \"data\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/pr_hist_entire_period.csv\"},\n",
    "        \"validation\": {\"start_datetime\": \"1996/01/01 00:00:00\",\n",
    "                       \"end_datetime\": \"2005/12/31 23:00:00\",\n",
    "                       \"data\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/pr_hist_entire_period.csv\"},\n",
    "        \"reference\": {\"start_datetime\": \"1970/01/01 00:00:00\",\n",
    "                      \"end_datetime\": \"2005/12/31 23:00:00\",\n",
    "                      \"data\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/pr_obs.csv\"},\n",
    "        \"output_name\": {\"model\": \"pr_hist_1970_2005.csv\", \"reference\": \"pr_obs_1970_2005.csv\"}},\n",
    "        {\"variable\": \"temperature\",\n",
    "         \"calibration\": {\"start_datetime\": \"1970/01/01 00:00:00\",\n",
    "                         \"end_datetime\": \"1995/12/31 23:00:00\",\n",
    "                         \"data\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/tas_hist_entire_period.csv\"},\n",
    "         \"validation\": {\"start_datetime\": \"1996/01/01 00:00:00\",\n",
    "                        \"end_datetime\": \"2005/12/31 23:00:00\",\n",
    "                        \"data\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/tas_hist_entire_period.csv\"},\n",
    "         \"reference\": {\"start_datetime\": \"1970/01/01 00:00:00\",\n",
    "                       \"end_datetime\": \"2005/12/31 23:00:00\",\n",
    "                       \"data\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/tas_obs.csv\"},\n",
    "         \"output_name\": {\"model\": \"tas_hist_1970_2005.csv\", \"reference\": \"tas_obs_1970_2005.csv\"}}\n",
    "    ],\n",
    "    \"output_path\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/\"\n",
    "}\n",
    "\n",
    "# HISTORICAL+PROJECTION CASE STUDY\n",
    "# 1970-2005 calibration + 2006-2050 validation == 1970-2005 historical timeseries + 2006-2050 project timeseries\n",
    "\n",
    "metadata_02 = {\n",
    "    \"testcase\": \"testcase_02\",\n",
    "    \"variables\": [\n",
    "        {\"variable\": \"precipitation\",\n",
    "         \"calibration\": {\"start_datetime\": \"1970/01/01 00:00:00\",\n",
    "                         \"end_datetime\": \"2005/12/31 23:00:00\",\n",
    "                         \"data\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/pr_hist_entire_period.csv\"},\n",
    "         \"validation\": {\"start_datetime\": \"2006/01/01 00:00:00\",\n",
    "                        \"end_datetime\": \"2050/12/31 23:00:00\",\n",
    "                        \"data\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/pr_project.csv\"},\n",
    "         \"reference\": {\"start_datetime\": \"1970/01/01 00:00:00\",\n",
    "                       \"end_datetime\": \"2005/12/31 23:00:00\",\n",
    "                       \"data\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/pr_obs.csv\"},\n",
    "         \"output_name\": {\"model\": \"pr_hist_proj_1970_2050.csv\", \"reference\": \"pr_obs_1970_2005.csv\"}},\n",
    "        {\"variable\": \"temperature\",\n",
    "         \"calibration\": {\"start_datetime\": \"1970/01/01 00:00:00\",\n",
    "                         \"end_datetime\": \"2005/12/31 23:00:00\",\n",
    "                         \"data\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/tas_hist_entire_period.csv\"},\n",
    "         \"validation\": {\"start_datetime\": \"1996/01/01 00:00:00\",\n",
    "                        \"end_datetime\": \"2050/12/31 23:00:00\",\n",
    "                        \"data\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/tas_project.csv\"},\n",
    "         \"reference\": {\"start_datetime\": \"1970/01/01 00:00:00\",\n",
    "                       \"end_datetime\": \"2005/12/31 23:00:00\",\n",
    "                       \"data\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/tas_obs.csv\"},\n",
    "         \"output_name\": {\"model\": \"tas_hist_proj_1970_2050.csv\", \"reference\": \"tas_obs_1970_2005.csv\"}\n",
    "         }\n",
    "    ],\n",
    "    \"output_path\": \"/media/windows/projects/bias_correction/applications/cmcc_cm/data/pre_processed_202206281324/\"\n",
    "}\n",
    "\n",
    "data_to_extract = [metadata_01, metadata_02]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta in data_to_extract:\n",
    "\n",
    "    curr_output_path = meta['output_path'] + meta['testcase']\n",
    "    mkNestedDir(curr_output_path)\n",
    "\n",
    "    for var in meta['variables']:\n",
    "        \n",
    "        start_cal = pd.to_datetime( var['calibration']['start_datetime'] )\n",
    "        end_cal = pd.to_datetime( var['calibration']['end_datetime'] )\n",
    "\n",
    "        curr_data_cal = pd.read_csv( var['calibration']['data'] )\n",
    "        curr_data_cal['datetime'] = pd.to_datetime( curr_data_cal['datetime'], format=\"%Y-%m-%d %H:%M:%S\" )\n",
    "        curr_data_cal.set_index('datetime', inplace=True)\n",
    "        curr_data_cal = curr_data_cal[start_cal:end_cal]\n",
    "\n",
    "        start_val = pd.to_datetime( var['validation']['start_datetime'] )\n",
    "        end_val = pd.to_datetime( var['validation']['end_datetime'] )\n",
    "\n",
    "        curr_data_val = pd.read_csv( var['validation']['data'] )\n",
    "        curr_data_val['datetime'] = pd.to_datetime( curr_data_val['datetime'], format=\"%Y-%m-%d %H:%M:%S\" )\n",
    "        curr_data_val.set_index('datetime', inplace=True)\n",
    "        curr_data_val = curr_data_val[start_val:end_val]\n",
    "\n",
    "        # merge the calibration and validation period\n",
    "        curr_data_model = pd.concat([ curr_data_cal,curr_data_val ])\n",
    "        \n",
    "        curr_data_model = round( curr_data_model, 2 )\n",
    "\n",
    "        # save the data\n",
    "        curr_data_model.index = [ dt.datetime.strftime(d,format=\"%Y-%m-%d %H:%M:%S\") for d in curr_data_model.index ]\n",
    "        curr_data_model.index.name = \"datetime\"\n",
    "        curr_data_model.to_csv( curr_output_path + \"/\" + var['output_name']['model'] )\n",
    "\n",
    "        del [curr_data_model]\n",
    "\n",
    "        start_ref = pd.to_datetime( var['reference']['start_datetime'] )\n",
    "        end_ref = pd.to_datetime( var['reference']['end_datetime'] )\n",
    "\n",
    "        curr_data_ref = pd.read_csv( var['reference']['data'] )\n",
    "        curr_data_ref['datetime'] = pd.to_datetime( curr_data_ref['datetime'], format=\"%Y-%m-%d %H:%M:%S\" )\n",
    "        curr_data_ref.set_index('datetime', inplace=True)\n",
    "        curr_data_ref = curr_data_ref[start_ref:end_ref]\n",
    "\n",
    "        curr_data_ref = round( curr_data_ref, 2 )\n",
    "\n",
    "        # save the data\n",
    "        curr_data_ref.index = [ dt.datetime.strftime(d,format=\"%Y-%m-%d %H:%M:%S\") for d in curr_data_ref.index ]\n",
    "        curr_data_ref.index.name = \"datetime\"\n",
    "        curr_data_ref.to_csv( curr_output_path + \"/\" + var['output_name']['reference'] )\n",
    "\n",
    "        del [curr_data_ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUILD THE CASE STUDY GRID\n",
    "\n",
    "start_metadata = \"/media/windows/projects/bias_correction/data/meteo/cmcc_cm/observations/Stations_for_gis.xlsx\"\n",
    "\n",
    "tmp_file = pd.read_excel(start_metadata,engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_file[['NAME']].iloc(0)[0].values[0].split(\" \")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_file.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID,East,North,Elev\n",
    "ids = []\n",
    "east = []\n",
    "north = []\n",
    "elevation = []\n",
    "\n",
    "metadata_std = pd.DataFrame()\n",
    "\n",
    "for el in tmp_file.index:\n",
    "    curr_id = tmp_file['NAME'].iloc[el].split(\" \")[-2]\n",
    "    if curr_id == \"TX\":\n",
    "        if tmp_file['NAME'].iloc[el][0:2] == \"SA\":\n",
    "            curr_id = \"TX_SA\"\n",
    "        else:\n",
    "            curr_id = \"TX_D\"\n",
    "    ids.append( curr_id )\n",
    "\n",
    "    east.append( tmp_file['LONGITUDE'].iloc[el] )\n",
    "    north.append( tmp_file['LATITUDE'].iloc[el] )\n",
    "    elevation.append( tmp_file['ELEVATION (m)'].iloc[el] )\n",
    "\n",
    "metadata_std['ID'] = ids\n",
    "metadata_std['East'] = east\n",
    "metadata_std['North'] = north\n",
    "metadata_std['Elev'] = elevation\n",
    "\n",
    "metadata_std.set_index('ID', inplace=True)\n",
    "metadata_std.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "metadata_std.to_csv( output_path + \"USA_grid.csv\" )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
