{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "lib_dir = \"/home/daniele/documents/github/ftt01/phd/share/lib/\"\n",
    "sys.path.insert( 0, lib_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rauth import OAuth2Service\n",
    "import requests\n",
    "\n",
    "class AlperiaOAuth2Client:\n",
    "    def __init__(self):\n",
    "        self.access_token = None\n",
    "\n",
    "        self.service = OAuth2Service(\n",
    "            name=\"ftt01\",\n",
    "            client_id=\"47wWdaYVrvZAXumaBYpMtnkkVp2toXsmaFpB7aqjzhQFCtAZQJgMrre9jCyR7tY4\",\n",
    "            client_secret=\"Ecw2yA2fTToM4ycwrEvAhgvtKRa6PrGR3EXBCZhTyW7MUyibBhJWs2vqi8vRucpU\",\n",
    "            access_token_url=\"https://gaia-api-noprod.alperia.digital/connect/token\",\n",
    "            authorize_url=\"https://gaia-api-noprod.alperia.digital/connect/token\",\n",
    "            base_url=\"https://gaia-api-noprod.alperia.digital/\",\n",
    "        )\n",
    "\n",
    "        self.get_access_token()\n",
    "\n",
    "    def get_access_token(self):\n",
    "        data = {\n",
    "                'grant_type': 'client_credentials', # generally required! \n",
    "               }\n",
    "\n",
    "        session = self.service.get_auth_session(data=data, decoder=json.loads)\n",
    "\n",
    "        self.access_token = session.access_token\n",
    "\n",
    "    def get_streamflow(self, basin, date_from, date_to):\n",
    "        base_url = \"https://gaia-api-noprod.alperia.digital/api/portate\"\n",
    "        params = {\n",
    "            \"Bacino\" : basin,\n",
    "            \"DateFrom\" : date_from,\n",
    "            \"DateTo\" : date_to\n",
    "        }\n",
    "        response = requests.get( base_url, params=params, headers={'Authorization': 'Bearer {myToken}'.format(myToken=self.access_token)})\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(\"Error: \" + str(response.status_code))\n",
    "            return []\n",
    "        \n",
    "        response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(json_data, output_filename):\n",
    "    datetime_vector = [pd.to_datetime(el['date'],utc=True) + dt.timedelta(hours=el['hour']) for el in json_data]\n",
    "    values_vector = [ float(el['value']) for el in json_data ]\n",
    "\n",
    "    df = pd.DataFrame(columns=['datetime','values'])\n",
    "    df[\"datetime\"] = datetime_vector\n",
    "    df[\"values\"] = values_vector\n",
    "\n",
    "    try:\n",
    "        old_data = pd.read_csv(output_filename)\n",
    "        old_data['datetime'] = pd.to_datetime(old_data['datetime'], format='%Y-%m-%d %H:%M:%S', utc=True )\n",
    "        old_data.set_index('datetime', inplace=True)\n",
    "\n",
    "        df = append_data(old_data,df)\n",
    "        df.reset_index(inplace=True)\n",
    "    except:\n",
    "        print('No old data!')\n",
    "\n",
    "    df['datetime'] = [ el.strftime('%Y-%m-%dT%H:%M:%SZ%z') for el in df['datetime'] ]\n",
    "\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df = df[['values']]\n",
    "\n",
    "    df.to_csv(output_filename)\n",
    "\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = AlperiaOAuth2Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = dt.datetime.today()\n",
    "\n",
    "output_path = \"/media/lacie2022/data/meteo/eu/it/taa/aa/alperia/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No old data!\n"
     ]
    }
   ],
   "source": [
    "## SB002\n",
    "output_filename = output_path + \"SB002.csv\"\n",
    "##\n",
    "json_data = pp.get_streamflow('Vasca Valdurna', '1900-01-01 00:00', end_date.strftime('%Y-%m-%d %H:%M'))\n",
    "save_to_csv(json_data, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No old data!\n"
     ]
    }
   ],
   "source": [
    "## SB003\n",
    "output_filename = output_path + \"SB003.csv\"\n",
    "##\n",
    "json_data = pp.get_streamflow('Vasca Corvara', '1900-01-01 00:00', end_date.strftime('%Y-%m-%d %H:%M'))\n",
    "save_to_csv(json_data, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No old data!\n"
     ]
    }
   ],
   "source": [
    "## SB004\n",
    "output_filename = output_path + \"SB004.csv\"\n",
    "##\n",
    "json_data = pp.get_streamflow('Vasca di Carico Enerpass', '1900-01-01 00:00', end_date.strftime('%Y-%m-%d %H:%M'))\n",
    "save_to_csv(json_data, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SB005 - dati provincia ma forniti validati\n",
    "output_filename = output_path + \"SB005.csv\"\n",
    "input_data_path = \"/media/lacie2022/data/meteo/eu/it/taa/aa/alperia/v3/aggiornamento_dati/validati-SB005_Pontives_orari.csv\"\n",
    "input_data = pd.read_csv(input_data_path, skiprows=0, index_col=0, parse_dates=True)\n",
    "\n",
    "local_tz = pytz.utc\n",
    "utc_tz = pytz.utc\n",
    "\n",
    "df = pd.DataFrame(columns=['datetime','values'])\n",
    "df['datetime'] = [ lt.astimezone(utc_tz) for lt in [ local_tz.localize(t) for t in input_data.index ]]\n",
    "### apply delay to the timeseries from UTC+1 to UTC\n",
    "df['datetime'] = [ el-dt.timedelta(hours=1) for el in df['datetime'] ]\n",
    "\n",
    "df['datetime'] = [ el.strftime('%Y-%m-%dT%H:%M:%SZ%z') for el in df['datetime'] ]\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "vals = []\n",
    "for v in input_data.values:\n",
    "    try:\n",
    "        act_val = round(float(v), 2)\n",
    "        if act_val > 0.01:\n",
    "            vals.append(act_val)\n",
    "        else:\n",
    "            raise\n",
    "    except:\n",
    "        vals.append(np.NaN)\n",
    "df['values'] = vals\n",
    "\n",
    "df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No old data!\n"
     ]
    }
   ],
   "source": [
    "## SB006\n",
    "output_filename = output_path + \"SB006.csv\"\n",
    "##\n",
    "json_data = pp.get_streamflow('Vasca di Rio Riva', '1900-01-01 00:00', end_date.strftime('%Y-%m-%d %H:%M'))\n",
    "save_to_csv(json_data, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SB007 - dati provincia ma forniti validati [UTC+1]\n",
    "output_filename = output_path + \"SB007.csv\"\n",
    "input_data_path = \"/media/lacie2022/data/meteo/eu/it/taa/aa/alperia/v3/aggiornamento_dati/validati-SB007_Trens_orari.csv\"\n",
    "\n",
    "input_data = pd.read_csv(input_data_path, skiprows=0, index_col=0, parse_dates=True)\n",
    "\n",
    "local_tz = pytz.utc\n",
    "utc_tz = pytz.utc\n",
    "\n",
    "df = pd.DataFrame(columns=['datetime','values'])\n",
    "df['datetime'] = [ lt.astimezone(utc_tz) for lt in [ local_tz.localize(t) for t in input_data.index ]]\n",
    "### apply delay to the timeseries from UTC+1 to UTC\n",
    "df['datetime'] = [ el-dt.timedelta(hours=1) for el in df['datetime']]\n",
    "\n",
    "df['datetime'] = [ el.strftime('%Y-%m-%dT%H:%M:%SZ%z') for el in df['datetime']]\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "vals = []\n",
    "for v in input_data.values:\n",
    "    try:\n",
    "        act_val = round(float(v), 2)\n",
    "        if act_val > 0.01:\n",
    "            vals.append(act_val)\n",
    "        else:\n",
    "            raise\n",
    "    except:\n",
    "        vals.append(np.NaN)\n",
    "df['values'] = vals\n",
    "\n",
    "df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No old data!\n"
     ]
    }
   ],
   "source": [
    "## SB008\n",
    "output_filename = output_path + \"SB008.csv\"\n",
    "# input_data_path = \"/media/lacie2022/data/meteo/eu/it/taa/aa/alperia/curon.csv\"\n",
    "# local_tz = pytz.timezone(\"Europe/Rome\")\n",
    "# utc_tz = pytz.utc\n",
    "\n",
    "# input_data = pd.read_csv(input_data_path, skiprows=0, header=None, index_col=0, parse_dates=True)\n",
    "json_data = pp.get_streamflow('Vasca Melago', '1900-01-01 00:00', end_date.strftime('%Y-%m-%d %H:%M'))\n",
    "save_to_csv(json_data, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(columns=['datetime','values'])\n",
    "# df['datetime'] = [ lt.astimezone(utc_tz) for lt in [ local_tz.localize(t) for t in input_data.index ]]\n",
    "# df['datetime'] = [ el.strftime('%Y-%m-%dT%H:%M:%SZ%z') for el in df['datetime']]\n",
    "# df.set_index('datetime', inplace=True)\n",
    "\n",
    "# vals = []\n",
    "# for v in input_data.values:\n",
    "#     try:\n",
    "#         act_val = round(float(v), 2)\n",
    "#         if act_val > 0.01:\n",
    "#             vals.append(act_val)\n",
    "#         else:\n",
    "#             raise\n",
    "#     except:\n",
    "#         vals.append(np.NaN)\n",
    "# df['values'] = vals\n",
    "\n",
    "# df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No old data!\n"
     ]
    }
   ],
   "source": [
    "## SB009\n",
    "output_filename = output_path + \"SB009.csv\"\n",
    "# input_data_path = \"/home/daniele/downloads/portate_castelbello_produzione.csv\"\n",
    "# local_tz = pytz.timezone(\"Europe/Rome\")\n",
    "# utc_tz = pytz.utc\n",
    "\n",
    "# input_data = pd.read_csv(input_data_path, header=None, parse_dates=True, index_col=0)\n",
    "# df = pd.DataFrame(columns=['datetime','values'])\n",
    "# df['datetime'] = [ lt.astimezone(utc_tz) for lt in [ local_tz.localize(t) for t in input_data.index ]]\n",
    "# df['datetime'] = [ el.strftime('%Y-%m-%dT%H:%M:%SZ%z') for el in df['datetime']]\n",
    "# df.set_index('datetime', inplace=True)\n",
    "\n",
    "# vals = []\n",
    "# for v in input_data.values:\n",
    "#     try:\n",
    "#         vals.append(round(float(v), 2))\n",
    "#     except:\n",
    "#         vals.append(np.NaN)\n",
    "# df['values'] = vals\n",
    "\n",
    "# df.to_csv(output_filename)\n",
    "json_data = pp.get_streamflow('Vasca Castelbello', '1900-01-01 00:00', end_date.strftime('%Y-%m-%d %H:%M'))\n",
    "save_to_csv(json_data, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_tz = pytz.timezone(\"Europe/Rome\")\n",
    "\n",
    "# start_date = dt.datetime( 2021, 8, 15, tzinfo=local_tz )\n",
    "# end_date = dt.datetime( 2021, 12, 12, tzinfo=local_tz )\n",
    "# # end_date = dt.datetime.today().replace(tzinfo=local_tz) + dt.timedelta( days=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### SB001\n",
    "# ## historical\n",
    "# plan_data_path = \"/media/lacie2022/data/meteo/eu/it/taa/aa/online/streamflow/20750PG.csv\"\n",
    "# plan_data = pd.read_csv( plan_data_path, sep=\",\", header=0 )\n",
    "\n",
    "# plan_data['datetime'] = pd.to_datetime(\n",
    "#     [ t.astimezone(local_tz).strftime(\"%Y-%m-%d %H:%M:%S\") for t in pd.to_datetime( plan_data['datetime'], format=\"%Y-%m-%dT%H:%M:%SZ%z\" ) ]\n",
    "# )\n",
    "# plan_data.set_index( 'datetime', inplace=True )\n",
    "# plan_data_cut = plan_data[ dt.datetime.strftime(start_date,\"%Y-%m-%d %H:%M:%S\"): dt.datetime.strftime(end_date,\"%Y-%m-%d %H:%M:%S\")]\n",
    "# plan_data_cut = resample_timeseries(df=plan_data_cut, offset=True, res_type='mean', step='1H')\n",
    "\n",
    "# ## forecast\n",
    "# forecast = pd.read_csv('/media/windows/projects/pp/SHARED/machine_learning/output/online/B001/SB001/R003/mean/forecast_1col.csv')\n",
    "# forecast['index_CET'] = pd.to_datetime( forecast['index_CET'], format=\"%Y-%m-%d %H:%M:%S\" )\n",
    "\n",
    "# forecast.set_index('index_CET', inplace=True)\n",
    "# forecast.index.name = 'datetime'\n",
    "# forecast_cut = forecast[ dt.datetime.strftime(start_date,\"%Y-%m-%d %H:%M:%S\"): dt.datetime.strftime(end_date,\"%Y-%m-%d %H:%M:%S\")]\n",
    "# forecast_cut = resample_timeseries(df=forecast_cut, offset=True, res_type='mean', step='1H')\n",
    "\n",
    "# forecast_cut['real'] = plan_data_cut['values']\n",
    "\n",
    "# forecast_cut.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### SB005\n",
    "# ## historical\n",
    "# plan_data_path = \"/media/lacie2022/data/meteo/eu/it/taa/aa/online/streamflow/73950PG.csv\"\n",
    "# plan_data = pd.read_csv( plan_data_path, sep=\",\", header=0 )\n",
    "\n",
    "# plan_data['datetime'] = pd.to_datetime(\n",
    "#     [ t.astimezone(local_tz).strftime(\"%Y-%m-%d %H:%M:%S\") for t in pd.to_datetime( plan_data['datetime'], format=\"%Y-%m-%dT%H:%M:%SZ%z\" ) ]\n",
    "# )\n",
    "# plan_data.set_index( 'datetime', inplace=True )\n",
    "# plan_data_cut = plan_data[ dt.datetime.strftime(start_date,\"%Y-%m-%d %H:%M:%S\"): dt.datetime.strftime(end_date,\"%Y-%m-%d %H:%M:%S\")]\n",
    "# plan_data_cut = resample_timeseries(df=plan_data_cut, offset=True, res_type='mean', step='1H')\n",
    "\n",
    "# ## forecast\n",
    "# forecast = pd.read_csv('/media/windows/projects/pp/SHARED/machine_learning/output/online/B001/SB005/R003/mean/forecast_1col.csv')\n",
    "# forecast['index_CET'] = pd.to_datetime( forecast['index_CET'], format=\"%Y-%m-%d %H:%M:%S\" )\n",
    "\n",
    "# forecast.set_index('index_CET', inplace=True)\n",
    "# forecast.index.name = 'datetime'\n",
    "# forecast_cut = forecast[ dt.datetime.strftime(start_date,\"%Y-%m-%d %H:%M:%S\"): dt.datetime.strftime(end_date,\"%Y-%m-%d %H:%M:%S\")]\n",
    "# forecast_cut = resample_timeseries(df=forecast_cut, offset=True, res_type='mean', step='1H')\n",
    "\n",
    "# forecast_cut['real'] = plan_data_cut['values']\n",
    "\n",
    "# forecast_cut.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### SB007\n",
    "# ## historical\n",
    "# plan_data_path = \"/media/lacie2022/data/meteo/eu/it/taa/aa/online/streamflow/37230PG.csv\"\n",
    "# plan_data = pd.read_csv( plan_data_path, sep=\",\", header=0 )\n",
    "\n",
    "# plan_data['datetime'] = pd.to_datetime(\n",
    "#     [ t.astimezone(local_tz).strftime(\"%Y-%m-%d %H:%M:%S\") for t in pd.to_datetime( plan_data['datetime'], format=\"%Y-%m-%dT%H:%M:%SZ%z\" ) ]\n",
    "# )\n",
    "# plan_data.set_index( 'datetime', inplace=True )\n",
    "# plan_data_cut = plan_data[ dt.datetime.strftime(start_date,\"%Y-%m-%d %H:%M:%S\"): dt.datetime.strftime(end_date,\"%Y-%m-%d %H:%M:%S\")]\n",
    "# plan_data_cut = resample_timeseries(df=plan_data_cut, offset=True, res_type='mean', step='1H')\n",
    "\n",
    "# ## forecast\n",
    "# forecast = pd.read_csv('/media/windows/projects/pp/SHARED/machine_learning/output/online/B001/SB007/R003/mean/forecast_1col.csv')\n",
    "# forecast['index_CET'] = pd.to_datetime( forecast['index_CET'], format=\"%Y-%m-%d %H:%M:%S\" )\n",
    "\n",
    "# forecast.set_index('index_CET', inplace=True)\n",
    "# forecast.index.name = 'datetime'\n",
    "# forecast_cut = forecast[ dt.datetime.strftime(start_date,\"%Y-%m-%d %H:%M:%S\"): dt.datetime.strftime(end_date,\"%Y-%m-%d %H:%M:%S\")]\n",
    "# forecast_cut = resample_timeseries(df=forecast_cut, offset=True, res_type='mean', step='1H')\n",
    "\n",
    "# forecast_cut['real'] = plan_data_cut['values']\n",
    "\n",
    "# forecast_cut.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals_as_float = []\n",
    "# for v in plan_data['values'].to_list():\n",
    "#     try:\n",
    "#         vals_as_float.append(float(v))\n",
    "#     except:\n",
    "#         vals_as_float.append(np.nan)\n",
    "\n",
    "# plan_data.loc[:,'values'] = vals_as_float\n",
    "\n",
    "# plan_data.resample('h').mean()\n",
    "\n",
    "# plan_data = plan_data[ start_date:end_date ]\n",
    "\n",
    "# plan_data.index = plan_data.index.strftime( \"%Y-%m-%d %H:%M:%S\" )\n",
    "\n",
    "# c_output_path = \"/home/daniele/documents/data/providers/it_taa_meteoaltoadige/merged/data/streamflow/\"\n",
    "# mkNestedDir(c_output_path)\n",
    "# # sarentino_corvara_data.to_csv( c_output_path + start_date.strftime('%Y%m%d') + end_date.strftime('%Y%m%d') + \".csv\" )\n",
    "# plan_data.to_csv( c_output_path + \"SB001.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan_data.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checked_data = data_check( plan_data, style='streamflow' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checked_data.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sarentino_data_path = \"/media/windows/projects/pp/SHARED/alperia/streamflow/sarentino.csv\"\n",
    "# sarentino_data = pd.read_csv( sarentino_data_path, sep=\",\", skiprows=1, header=None )[[0,1,2]]\n",
    "\n",
    "# sarentino_corvara_data = sarentino_data[[0,1]]\n",
    "# sarentino_valdurna_data = sarentino_data[[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dt = []\n",
    "# for r in sarentino_corvara_data[0].to_list():\n",
    "#     date_time = r.split(\" \")\n",
    "#     day_month_year = date_time[0].split(\"/\")\n",
    "#     hour_secs = date_time[1].split(\":\")\n",
    "\n",
    "#     new_dt.append( dt.datetime( \n",
    "#         int(day_month_year[2]), \n",
    "#         int(day_month_year[0]),\n",
    "#         int(day_month_year[1]),\n",
    "#         int(hour_secs[0]),\n",
    "#         int(hour_secs[1]),\n",
    "#         tzinfo=pytz.timezone(\"Europe/Rome\")\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# sarentino_corvara_data.loc[:,'datetime'] = new_dt\n",
    "# sarentino_corvara_data.loc[:,'values'] = sarentino_corvara_data[1]\n",
    "# sarentino_corvara_data = sarentino_corvara_data[['datetime','values']]\n",
    "# sarentino_corvara_data.set_index('datetime', inplace=True)\n",
    "\n",
    "# vals_as_float = []\n",
    "# for v in sarentino_corvara_data['values'].to_list():\n",
    "#     try:\n",
    "#         vals_as_float.append(float(v))\n",
    "#     except:\n",
    "#         vals_as_float.append(np.nan)\n",
    "\n",
    "# sarentino_corvara_data.loc[:,'values'] = vals_as_float\n",
    "\n",
    "# sarentino_corvara_data.resample('h').mean()\n",
    "\n",
    "# sarentino_corvara_data = sarentino_corvara_data[ start_date:end_date ]\n",
    "\n",
    "# sarentino_corvara_data.index = sarentino_corvara_data.index.strftime( \"%Y-%m-%d %H:%M:%S\" )\n",
    "\n",
    "# c_output_path = \"/home/daniele/documents/data/providers/it_taa_meteoaltoadige/merged/data/streamflow/\"\n",
    "# mkNestedDir(c_output_path)\n",
    "# # sarentino_corvara_data.to_csv( c_output_path + start_date.strftime('%Y%m%d') + end_date.strftime('%Y%m%d') + \".csv\" )\n",
    "# sarentino_corvara_data.to_csv( c_output_path + \"SB002.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sarentino_corvara_data.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dt = []\n",
    "# for r in sarentino_valdurna_data[0].to_list():\n",
    "#     date_time = r.split(\" \")\n",
    "#     day_month_year = date_time[0].split(\"/\")\n",
    "#     hour_secs = date_time[1].split(\":\")\n",
    "\n",
    "#     new_dt.append( dt.datetime( \n",
    "#         int(day_month_year[2]), \n",
    "#         int(day_month_year[0]),\n",
    "#         int(day_month_year[1]),\n",
    "#         int(hour_secs[0]),\n",
    "#         int(hour_secs[1]),\n",
    "#         tzinfo=pytz.timezone(\"Europe/Rome\")\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# sarentino_valdurna_data.loc[:,'datetime'] = new_dt\n",
    "# sarentino_valdurna_data.loc[:,'values'] = sarentino_valdurna_data[2]\n",
    "# sarentino_valdurna_data = sarentino_valdurna_data[['datetime','values']]\n",
    "# sarentino_valdurna_data.set_index('datetime', inplace=True)\n",
    "\n",
    "# vals_as_float = []\n",
    "# for v in sarentino_valdurna_data['values'].to_list():\n",
    "#     try:\n",
    "#         vals_as_float.append(float(v))\n",
    "#     except:\n",
    "#         vals_as_float.append(np.nan)\n",
    "\n",
    "# sarentino_valdurna_data.loc[:,'values'] = vals_as_float\n",
    "\n",
    "# sarentino_valdurna_data.resample('h').mean()\n",
    "\n",
    "# sarentino_valdurna_data = sarentino_valdurna_data[ start_date:end_date ]\n",
    "\n",
    "# sarentino_valdurna_data.index = sarentino_valdurna_data.index.strftime( \"%Y-%m-%d %H:%M:%S\" )\n",
    "\n",
    "# c_output_path = \"/home/daniele/documents/data/providers/it_taa_meteoaltoadige/merged/data/streamflow/\"\n",
    "# mkNestedDir(c_output_path)\n",
    "# # sarentino_valdurna_data.to_csv( c_output_path + start_date.strftime('%Y%m%d') + end_date.strftime('%Y%m%d') + \".csv\" )\n",
    "# sarentino_valdurna_data.to_csv( c_output_path + \"SB003.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sarentino_valdurna_data.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enerpass_data_path = \"/media/windows/projects/pp/SHARED/alperia/streamflow/enerpass.csv\"\n",
    "# enerpass_data = pd.read_csv( enerpass_data_path, sep=\"\\t\", skiprows=1, header=None )[[0,1]]\n",
    "\n",
    "# new_dt = []\n",
    "# for r in enerpass_data[0].to_list():\n",
    "#     date_time = r.split(\" \")\n",
    "#     day_month_year = date_time[0].split(\"/\")\n",
    "#     hour_secs = date_time[1].split(\":\")\n",
    "\n",
    "#     new_dt.append( dt.datetime( \n",
    "#         int(day_month_year[2]), \n",
    "#         int(day_month_year[0]),\n",
    "#         int(day_month_year[1]),\n",
    "#         int(hour_secs[0]),\n",
    "#         int(hour_secs[1]),\n",
    "#         tzinfo=pytz.timezone(\"Europe/Rome\")\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# enerpass_data.loc[:,'datetime'] = new_dt\n",
    "# enerpass_data.loc[:,'values'] = enerpass_data[1]\n",
    "# enerpass_data = enerpass_data[['datetime','values']]\n",
    "# enerpass_data.set_index('datetime', inplace=True)\n",
    "\n",
    "# enerpass_data.resample('h').apply(\n",
    "#     lambda x: np.nan if x.isnull().sum() > 0 else x.mean())\n",
    "\n",
    "# enerpass_data = enerpass_data[ start_date:end_date ]\n",
    "\n",
    "# enerpass_data.index = enerpass_data.index.strftime( \"%Y-%m-%d %H:%M:%S\" )\n",
    "\n",
    "# c_output_path = \"/home/daniele/documents/data/providers/it_taa_meteoaltoadige/merged/data/streamflow/\"\n",
    "# mkNestedDir(c_output_path)\n",
    "# # enerpass_data.to_csv( c_output_path + start_date.strftime('%Y%m%d') + end_date.strftime('%Y%m%d') + \".csv\" )\n",
    "# enerpass_data.to_csv( c_output_path + \"SB004.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enerpass_data.plot(figsize=(10,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enerpass_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
