{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# to link the lib in py scripts as well\n",
    "lib_dir = \"/home/daniele/documents/github/ftt01/phd/projects/hydrological_forecasting/\"\n",
    "\n",
    "os.chdir( lib_dir )\n",
    "sys.path.insert( 0, os.path.join(os.path.abspath(os.getcwd()),'lib') )\n",
    "\n",
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = \"/media/windows/projects/hydro_forecasting/machine_learning/\"\n",
    "\n",
    "variables = ['precipitation', 'temperature', 'streamflow']\n",
    "basin = \"passirio\"\n",
    "\n",
    "start_date_str = '2021-06-01 00:00:00'\n",
    "end_date_str = '2021-10-17 00:00:00'\n",
    "timezone_str = 'Europe/Rome'\n",
    "\n",
    "start_date = dt.datetime.strptime(start_date_str, '%Y-%m-%d %H:%M:%S').replace(tzinfo=ZoneInfo(timezone_str))\n",
    "end_date = dt.datetime.strptime(end_date_str, '%Y-%m-%d %H:%M:%S').replace(tzinfo=ZoneInfo(timezone_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_weather_data_path = wdir + \"data/observed/plan/\"\n",
    "\n",
    "### stations metadata\n",
    "metadata_data_path =  wdir + \"data/AA_weather_data/AA_stations_metadata.csv\"\n",
    "\n",
    "metadata = pd.read_csv( metadata_data_path )\n",
    "metadata = metadata[ metadata['ID_UI'] != '-999' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weather_data_path = \"/media/windows/data/meteo/eu/it/taa/aa/weather_stations/original/self_download/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in variables:\n",
    "\n",
    "    st_list = []\n",
    "    for m in metadata.index:\n",
    "\n",
    "        internal_id = str(metadata.loc[m]['ID'])\n",
    "        east = str(metadata.loc[m]['East'])\n",
    "        north = str(metadata.loc[m]['North'])\n",
    "        elevation = str(metadata.loc[m]['Elevation'])\n",
    "        id_ui = str(metadata.loc[m]['ID_UI'])\n",
    "\n",
    "        current_data_path = original_weather_data_path + \\\n",
    "            variable + \"/\" + str(id_ui) + \".csv\"\n",
    "        # print(current_data_path)\n",
    "        try:\n",
    "            current_data = pd.read_csv(\n",
    "                current_data_path, parse_dates=True, index_col=0)\n",
    "            current_data.index = current_data.index.tz_convert(timezone_str)\n",
    "\n",
    "            if variable == 'precipitation':\n",
    "                current_data = current_data.resample('h').sum()\n",
    "            elif variable == 'temperature':\n",
    "                current_data = current_data.resample('h').mean()\n",
    "            elif variable == 'streamflow':\n",
    "                current_data = current_data.resample('h').mean()\n",
    "            else:\n",
    "                print('NOT A VALID VARIABLE!')\n",
    "                continue\n",
    "\n",
    "            current_data = current_data[start_date:end_date]\n",
    "        except:\n",
    "            # print(\"MISSING: \" + current_data_path)\n",
    "            continue\n",
    "\n",
    "        current_data.index = [dt.datetime.strftime(\n",
    "            i, format=\"%Y-%m-%d %H:%M:%S\") for i in current_data.index]\n",
    "\n",
    "        df = current_data.to_csv(header=False).strip('\\n').split('\\n')\n",
    "        # <= this is the string that you can use with md5\n",
    "        data = '\\r\\n'.join(df)\n",
    "        # data = df_string.encode('utf8')  # <= this is bytes object to write the file\n",
    "        # print(df_bytes)\n",
    "\n",
    "        current_output_file = output_weather_data_path + \\\n",
    "            variable + \"/\" + internal_id + \".txt\"\n",
    "        mkNestedDir(getPathFromFilepath(current_output_file))\n",
    "\n",
    "        # ID,2.0\n",
    "        # x,642993.5\n",
    "        # y,5164882.0\n",
    "        # z,630.0\n",
    "        header = \"\"\"ID,{id}\\nx,{x}\\ny,{y}\\nz,{z}\\n\"\"\"\n",
    "\n",
    "        with open(current_output_file, 'w') as new:\n",
    "            new.write(header.format(id=internal_id,\n",
    "                      x=east, y=north, z=elevation))\n",
    "            new.write(data)\n",
    "\n",
    "        # add station to station list\n",
    "        st_props = {}\n",
    "\n",
    "        st_props[\"station_id\"] = internal_id\n",
    "        st_props[\"station_name\"] = str(metadata.loc[m]['Station']).lower().replace(\n",
    "            \" \", \"_\").replace(\"-\", \"_\").replace(\"(\", \"_\").replace(\")\", \"_\")\n",
    "        st_props[\"east\"] = east\n",
    "        st_props[\"north\"] = north\n",
    "        st_props[\"elevation\"] = elevation\n",
    "\n",
    "        st_list.append(st_props)\n",
    "\n",
    "    with open(output_weather_data_path + variable + \".json\", \"w\") as data_file:\n",
    "        json.dump(st_list, data_file, indent=4, sort_keys=True)\n",
    "\n",
    "        # print(current_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### build grid\n",
    "## is necessary to create the file output from the extracted data and local DTM\n",
    "\n",
    "grid_file = wdir + \"data/\" + basin + \"/output.csv\"\n",
    "grid_metadata = pd.read_csv( grid_file, index_col=1 )\n",
    "# grid_metadata.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "grid_metadata.dropna(inplace=True)\n",
    "\n",
    "st_list = []\n",
    "\n",
    "for id in grid_metadata.index:\n",
    "\n",
    "    st_props = {}\n",
    "\n",
    "    st_props[\"station_id\"] = str(id)\n",
    "    st_props[\"station_name\"] = str(id)\n",
    "    st_props[\"east\"] = str(grid_metadata.loc[id]['lat'])\n",
    "    st_props[\"north\"] = str(grid_metadata.loc[id]['lon'])\n",
    "    st_props[\"elevation\"] = str(grid_metadata.loc[id]['elevation'])\n",
    "\n",
    "    st_list.append( st_props )\n",
    "\n",
    "with open( wdir + \"data/\" + basin + \"/grid.json\", \"w\") as data_file:\n",
    "    json.dump(st_list, data_file, indent=4, sort_keys=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
