{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ DATA AND CONVERT INTO CSV\n",
    "1. setup the wdir to the directory where the grib2 data is collected\n",
    "2. each directory of the wdir will be opened and the grib inside it processed\n",
    "    a. each grib file will be converted in a grid, based on DWD weights\n",
    "    b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = \"/home/daniele/documents/github/ftt01/phd/projects/hydrological_forecasting/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTs\n",
    "import sys, os\n",
    "import glob\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from dask import dataframe as dd\n",
    "\n",
    "# to link the lib in py scripts as well\n",
    "os.chdir( wdir )\n",
    "sys.path.insert( 0, os.path.join(os.path.abspath(os.getcwd()),'lib') )\n",
    "\n",
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "data_path = \"/media/windows/projects/hydrological_forecasting/machine_learning/data/forecast/icon-d2-eps/\"\n",
    "\n",
    "# variable = 'tot_prec'\n",
    "variable = 't_2m'\n",
    "lead_hours = 2\n",
    "ensemble_number = 20\n",
    "\n",
    "current_hour = dt.datetime.now().hour\n",
    "\n",
    "if current_hour >= 3 and current_hour < 9:\n",
    "    h = '00'\n",
    "elif current_hour >= 9 and current_hour < 15:\n",
    "    h = '06'\n",
    "elif current_hour >= 15 and current_hour < 21:\n",
    "    h = '12'\n",
    "elif current_hour >= 21:\n",
    "    h = '18'\n",
    "else:\n",
    "    h = '18'\n",
    "\n",
    "init_time = dt.datetime.strftime( dt.datetime.now(), format='%Y-%m-%d' ) + 'T' + h + ':00:00'\n",
    "\n",
    "## Passirio basin\n",
    "lat = ( 46.68, 46.945 )\n",
    "lon = ( 11.015, 11.38 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_to_save = data_path + 'tmp/'\n",
    "\n",
    "mkNestedDir(dir_to_save)\n",
    "os.chdir(dir_to_save)\n",
    "\n",
    "download_process = '''docker run --rm --volume $(pwd):/local \\\n",
    "    deutscherwetterdienst/downloader downloader \\\n",
    "    --model icon-d2-eps \\\n",
    "    --single-level-fields t_2m \\\n",
    "    --max-time-step {lead_hours} \\\n",
    "    --timestamp {init_time} \\\n",
    "    --directory /local'''.format(lead_hours=lead_hours, init_time=init_time)\n",
    "\n",
    "print(download_process)\n",
    "\n",
    "# download the data\n",
    "subprocess.run(download_process,\n",
    "               shell=True, check=True,\n",
    "               executable='/bin/bash')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = glob.glob( data_path + '*/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in dirs:\n",
    "\n",
    "    os.chdir(el)\n",
    "\n",
    "    # regrid of the GRIBs\n",
    "    subprocess.run('''docker run --rm \\\n",
    "        --volume $(pwd):/local \\\n",
    "        --env INPUT_FILE=/local \\\n",
    "        --env OUTPUT_FILE=/local \\\n",
    "        deutscherwetterdienst/regrid:icon-d2-eps \\\n",
    "        /convert.sh''',\n",
    "                   shell=True, check=True,\n",
    "                   executable='/bin/bash')\n",
    "\n",
    "    for n in range(1, lead_hours+1):\n",
    "\n",
    "        n = str(n).zfill(3)\n",
    "\n",
    "        # create and move into the current_file_path\n",
    "        current_file_path = el + n + '/'\n",
    "        mkNestedDir(current_file_path)\n",
    "        os.chdir(current_file_path)\n",
    "\n",
    "        # identify and move current_file into current_file_path\n",
    "        current_file = glob.glob(\n",
    "            el + '*regridded*' + n + '*' + variable + '*.grib2')[0]\n",
    "\n",
    "        # print(current_file)\n",
    "\n",
    "        mv_process = \"mv {} {}\".format(\n",
    "            current_file, current_file_path + os.path.basename(current_file))\n",
    "        subprocess.run(mv_process, shell=True,\n",
    "                       check=True, executable='/bin/bash')\n",
    "\n",
    "        # extract data to output.csv\n",
    "        extraction_process = '''docker run --rm --volume $(pwd):/local \\\n",
    "            deutscherwetterdienst/python-eccodes grib_get_data -p date,time,stepRange,shortName {} > output.csv'''\n",
    "        extraction_process = extraction_process.format(\n",
    "            os.path.basename(current_file))\n",
    "        subprocess.run(extraction_process, shell=True,\n",
    "                       check=True, executable='/bin/bash')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_time_array = []\n",
    "for n in range(1, lead_hours+1):\n",
    "    n = str(n).zfill(3)\n",
    "    lead_time_array.append(n)\n",
    "\n",
    "full_data = pd.DataFrame(columns=['ID', 'lat', 'lon'] + lead_time_array)\n",
    "\n",
    "for el in dirs:\n",
    "\n",
    "    os.chdir(el)\n",
    "\n",
    "    for n in range(1, lead_hours+1):\n",
    "\n",
    "        n = str(n).zfill(3)\n",
    "\n",
    "        # create and move into the current_file_path\n",
    "        current_file_path = el + n + '/'\n",
    "\n",
    "        # read exported data and cut to the ROI\n",
    "        data_df = dd.read_csv(current_file_path + 'output.csv', sep='\\s+', header=None, skiprows=1,\n",
    "                              names=['lat', 'lon', 'values', 'date', 'time', 'step_range', 'name'], comment=\"L\")\n",
    "        \n",
    "        data_df = data_df.astype({'lat': float, 'lon': float, 'values': float,\n",
    "                                  'date': str, 'time': str, 'step_range': str, 'name': str})\n",
    "\n",
    "        data_df = data_df[data_df['lat'] >= lat[0]]\n",
    "        data_df = data_df[data_df['lat'] <= lat[1]]\n",
    "        data_df = data_df[data_df['lon'] >= lon[0]]\n",
    "        data_df = data_df[data_df['lon'] <= lon[1]]\n",
    "\n",
    "        data_df = data_df.compute()\n",
    "\n",
    "        # export data to a new structure\n",
    "        interruptor = int(len(data_df) / ensemble_number)\n",
    "        # print(interruptor)\n",
    "\n",
    "        full_data['ID'] = range(1, interruptor+1)\n",
    "        full_data.set_index('ID', inplace=True)\n",
    "\n",
    "        metadata = True\n",
    "        for ens in range(ensemble_number):\n",
    "\n",
    "            m = str(ens+1).zfill(3)\n",
    "            ens_file_path = current_file_path + m + '/'\n",
    "            mkNestedDir(ens_file_path)\n",
    "    #\n",
    "            # current_data = pd.DataFrame(columns=['ID', 'lat', 'lon', 'values'])\n",
    "            # ids = []\n",
    "            # lats = []\n",
    "            # lons = []\n",
    "            # vals = []\n",
    "    #\n",
    "            for i in range(interruptor*ens, interruptor*(1+ens)):\n",
    "\n",
    "                point_id = i + 1 - interruptor*ens\n",
    "    #           \n",
    "                # ids.append( point_id )\n",
    "                # lats.append(data_df.iloc[i]['lat'])\n",
    "                # lons.append(data_df.iloc[i]['lon'])\n",
    "                # vals.append(data_df.iloc[i]['values'])\n",
    "    #\n",
    "                if metadata == True:\n",
    "                    full_data.loc[point_id]['lat'] = data_df.iloc[i]['lat']\n",
    "                    full_data.loc[point_id]['lon'] = data_df.iloc[i]['lon']\n",
    "\n",
    "                if ens == 0:\n",
    "                    full_data.loc[point_id][n] = [data_df.iloc[i]['values']]\n",
    "                else:\n",
    "                    full_data.loc[point_id][n] = full_data.loc[point_id][n] + \\\n",
    "                        [data_df.iloc[i]['values']]\n",
    "\n",
    "                # print(data_df.iloc[i]['values'])\n",
    "    #\n",
    "            # current_data['ID'] = ids\n",
    "            # current_data['lat'] = lats\n",
    "            # current_data['lon'] = lons\n",
    "            # current_data['values'] = vals\n",
    "\n",
    "            # current_data.to_csv(ens_file_path + 'output.csv')\n",
    "\n",
    "            metadata = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.datetime.strptime( init_time, '%Y-%m-%dT%H:%M:%S' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = []\n",
    "dates = []\n",
    "\n",
    "for n in range(1, lead_hours+1):\n",
    "    \n",
    "    dates.append( dt.datetime.strptime( init_time, '%Y-%m-%dT%H:%M:%S' ) + dt.timedelta(hours=n) )\n",
    "\n",
    "    values = []\n",
    "    for el in full_data[str(n).zfill(3)]:\n",
    "        values = values + el\n",
    "\n",
    "    series.append( np.mean( values ) - 273.15 )\n",
    "\n",
    "test = pd.DataFrame( series, index=dates, columns=['values'] )\n",
    "test.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.to_csv( 'output_ariele.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker run --rm --volume $(pwd):/local \\\n",
    "#     deutscherwetterdienst/downloader downloader \\\n",
    "#     --model icon \\\n",
    "#     --single-level-fields t_2m,tot_prec \\\n",
    "#     --max-time-step 5 \\\n",
    "#     --directory /local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run --rm \\\n",
    "#     --volume $(pwd):/local \\\n",
    "#     --env INPUT_FILE=/local \\\n",
    "#     --env OUTPUT_FILE=/local \\\n",
    "#     deutscherwetterdienst/regrid:icon-eu-eps \\\n",
    "#     /convert.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"/media/windows/projects/hydrological_forecasting/machine_learning/data/forecast/icon-d2-eps/tmp/\")\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker run --rm --mount type=bind,source=\"$(pwd)\"/,target=/local deutscherwetterdienst/python-eccodes grib_ls icon-d2-eps_germany_icosahedral_single-level_2021102700_000_2d_t_2m.grib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
