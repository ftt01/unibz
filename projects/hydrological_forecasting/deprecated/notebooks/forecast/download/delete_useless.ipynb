{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ DATA AND CONVERT INTO CSV\n",
    "1. setup the wdir to the directory where the grib2 data is collected\n",
    "2. each directory of the wdir will be opened and the grib inside it processed\n",
    "    a. each grib file will be converted in a grid, based on DWD weights\n",
    "    b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = \"/home/daniele/documents/github/ftt01/phd/projects/hydrological_forecasting/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTs\n",
    "import sys, os\n",
    "import glob\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from dask import dataframe as dd\n",
    "\n",
    "# to link the lib in py scripts as well\n",
    "os.chdir( wdir )\n",
    "sys.path.insert( 0, os.path.join(os.path.abspath(os.getcwd()),'lib') )\n",
    "\n",
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def regrid_dir(path_dir, regridded=False):\n",
    "\n",
    "#     if regridded == False:\n",
    "\n",
    "#         os.chdir(path_dir)\n",
    "\n",
    "#         subprocess.run('''docker run --rm \\\n",
    "#             --volume $(pwd):/local \\\n",
    "#             --env INPUT_FILE=/local \\\n",
    "#             --env OUTPUT_FILE=/local \\\n",
    "#             deutscherwetterdienst/regrid:icon-d2-eps \\\n",
    "#             /convert.sh''',\n",
    "#                        shell=True, check=True,\n",
    "#                        executable='/bin/bash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform(file_path, destination):\n",
    "\n",
    "#     copy_process = \"cp {} {}\".format( file_path, destination + os.path.basename(file_path))\n",
    "#     subprocess.run(copy_process, shell=True,\n",
    "#                    check=True, executable='/bin/bash')\n",
    "\n",
    "#     os.chdir(os.path.dirname(destination))\n",
    "\n",
    "#     filename = os.path.basename(file_path)\n",
    "\n",
    "#     transform_command = '''docker run --rm \\\n",
    "#         --volume $(pwd):/local \\\n",
    "#             deutscherwetterdienst/python-eccodes \\\n",
    "#                 grib_copy -B stepRange {filename} temporal_{filename}'''\n",
    "#     transform_command = transform_command.format(filename=filename)\n",
    "#     print(transform_command)\n",
    "#     subprocess.run(transform_command,\n",
    "#                    shell=True, check=True,\n",
    "#                    executable='/bin/bash')\n",
    "    \n",
    "#     subprocess.run('''rm {}'''.format(filename),\n",
    "#                    shell=True, check=True,\n",
    "#                    executable='/bin/bash')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "# data_path = '/mnt/e/projects/hydrological_forecasting/machine_learning/data/forecast/icon-d2-eps_45h/'\n",
    "# data_path = '/media/windows/projects/hydrological_forecasting/machine_learning/data/forecast/icon-d2-eps_45h/done/'\n",
    "data_path = '/media/windows/projects/hydro_forecasting/machine_learning/data/forecast/icon-d2-eps_45h/todo/'\n",
    "\n",
    "# dummy_output = wdir + 'data/passirio/dummy_output.csv'\n",
    "\n",
    "variables = ['t_2m','tot_prec']\n",
    "init_ref = '03'\n",
    "lead_hours = 45\n",
    "ensemble_number = 20\n",
    "\n",
    "## Passirio basin\n",
    "basin = 'passirio'\n",
    "\n",
    "# ## Plan basin\n",
    "# basin = 'plan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = glob.glob( data_path + '*/' )\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Include standard modules\n",
    "# import argparse\n",
    "\n",
    "# # Initiate the parser\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# # Add long and short argument\n",
    "# parser.add_argument(\"--path\", \"-p\", help=\"set input path\")\n",
    "\n",
    "# # Read arguments from the command line\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# # Check for --path\n",
    "# if args.path:\n",
    "#     print(\"Set path to %s\" % args.path)\n",
    "#     dirs = [args.path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_time_array = []\n",
    "for n in range(1, lead_hours+1):\n",
    "    n = str(n).zfill(3)\n",
    "    lead_time_array.append(n)\n",
    "\n",
    "for el in dirs:\n",
    "\n",
    "    dir_gribs = glob.glob(el + '*.grib2')\n",
    "\n",
    "    if len(dir_gribs) != 0:\n",
    "        for dir_grib in dir_gribs:\n",
    "            rm_process = \"rm {}\".format(dir_grib)\n",
    "            print(rm_process)\n",
    "            subprocess.run(rm_process, shell=True,\n",
    "                           check=True, executable='/bin/bash')\n",
    "\n",
    "    # print(el)\n",
    "    tmp = el.split('/')\n",
    "    current_date = str(tmp[len(tmp) - 2])\n",
    "\n",
    "    # print(\"Current date: \" + current_date)\n",
    "\n",
    "    for variable in variables:\n",
    "\n",
    "        # print(\"Variable: \" + variable)\n",
    "\n",
    "        #current_file = []\n",
    "        for n in range(1, lead_hours+1):\n",
    "\n",
    "            n = str(n).zfill(3)\n",
    "            # print(\"Lead hour: \" + n)\n",
    "\n",
    "            # create inner dirs structure\n",
    "            current_file_path = el + n + '/' + basin + '/' + variable + '/'\n",
    "            # print(\"Current dir: \" + current_file_path)\n",
    "\n",
    "            original_current_file = glob.glob(current_file_path)\n",
    "\n",
    "            if len(original_current_file) != 0:\n",
    "                gribs = glob.glob(original_current_file[0] + '*.grib2')\n",
    "\n",
    "                if len(gribs) != 0:\n",
    "                    for grib in gribs:\n",
    "\n",
    "                        rm_process = \"rm {}\".format(grib)\n",
    "                        print(rm_process)\n",
    "                        subprocess.run(rm_process, shell=True,\n",
    "                                       check=True, executable='/bin/bash')\n",
    "                # else:\n",
    "                #     print('Missing GRIB2 files' + current_file_path)\n",
    "\n",
    "                output_completes = glob.glob(\n",
    "                    original_current_file[0] + 'output.csv')\n",
    "\n",
    "                if len(output_completes) != 0:\n",
    "                    for output_complete in output_completes:\n",
    "\n",
    "                        rm_process = \"rm {}\".format(output_complete)\n",
    "                        print(rm_process)\n",
    "                        subprocess.run(rm_process, shell=True,\n",
    "                                       check=True, executable='/bin/bash')\n",
    "                # else:\n",
    "                    # print('Missing OUTPUT files' + current_file_path)\n",
    "\n",
    "            # else:\n",
    "            #     print('Missing DIR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = range(1, ensemble_number + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consistency check\n",
    "\n",
    "for el in dirs:\n",
    "\n",
    "    tmp = el.split('/')\n",
    "    current_date = str(tmp[len(tmp) - 2])\n",
    "\n",
    "    for variable in variables:\n",
    "\n",
    "        for n in range(1, lead_hours+1):\n",
    "\n",
    "            n = str(n).zfill(3)\n",
    "\n",
    "            for ensemble in ensembles:\n",
    "\n",
    "                ensemble = str(ensemble).zfill(3)\n",
    "\n",
    "                file_to_check = el + n + '/' + basin + '/' + variable + '/' + ensemble + '/' + 'output.csv'\n",
    "\n",
    "                if not( os.path.isfile(file_to_check) ):\n",
    "                    print(\"Current date: \" + current_date)\n",
    "                    print(\"Variable: \" + variable)\n",
    "                    print(\"MISSING: \" + file_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
