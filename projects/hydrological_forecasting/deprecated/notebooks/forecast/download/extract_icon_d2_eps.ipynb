{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ DATA AND CONVERT INTO CSV\n",
    "1. setup the wdir to the directory where the grib2 data is collected\n",
    "2. each directory of the wdir will be opened and the grib inside it processed\n",
    "    - each grib file will be converted in a grid, based on DWD weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "lib_dir = \"/home/daniele/documents/github/ftt01/phd/share/lib\"\n",
    "sys.path.insert( 0, lib_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wdir = \"/home/daniele/documents/github/ftt01/phd/projects/hydrological_forecasting/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTs\n",
    "import sys, os\n",
    "import glob\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from dask import dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid_dir(path_dir, regridded=False):\n",
    "\n",
    "    if regridded == False:\n",
    "\n",
    "        os.chdir(path_dir)\n",
    "\n",
    "        subprocess.run('''docker run --rm \\\n",
    "            --volume $(pwd):/local \\\n",
    "            --env INPUT_FILE=/local \\\n",
    "            --env OUTPUT_FILE=/local \\\n",
    "            deutscherwetterdienst/regrid:icon-d2-eps \\\n",
    "            /convert.sh''',\n",
    "                       shell=True, check=True,\n",
    "                       executable='/bin/bash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(file_path, destination):\n",
    "\n",
    "    copy_process = \"cp {} {}\".format( file_path, destination + os.path.basename(file_path))\n",
    "    subprocess.run(copy_process, shell=True,\n",
    "                   check=True, executable='/bin/bash')\n",
    "\n",
    "    os.chdir(os.path.dirname(destination))\n",
    "\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    transform_command = '''docker run --rm \\\n",
    "        --volume $(pwd):/local \\\n",
    "            deutscherwetterdienst/python-eccodes \\\n",
    "                grib_copy -B stepRange {filename} temporal_{filename}'''\n",
    "    transform_command = transform_command.format(filename=filename)\n",
    "    print(transform_command)\n",
    "    subprocess.run(transform_command,\n",
    "                   shell=True, check=True,\n",
    "                   executable='/bin/bash')\n",
    "    \n",
    "    subprocess.run('''rm {}'''.format(filename),\n",
    "                   shell=True, check=True,\n",
    "                   executable='/bin/bash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "# data_path = '/media/local_ssd/projects//tmp/hydrological_forecasting/machine_learning/data/forecast/icon-d2-eps_45h/input/todo/'\n",
    "# output_path = '/media/local_ssd/projects//tmp/hydrological_forecasting/machine_learning/data/forecast/icon-d2-eps_45h/output_passirio/'\n",
    "data_path = '/media/local_ssd/projects/alperia/db/data/tmp/'\n",
    "output_path = '/media/local_ssd/projects/alperia/db/data/tmp/output/'\n",
    "\n",
    "dummy_output = output_path + 'dummy_output.csv'\n",
    "\n",
    "variables = ['tot_prec','t_2m']\n",
    "init_ref = '03'\n",
    "init_lead_time = 1\n",
    "lead_hours = 45\n",
    "ensemble_number = 20\n",
    "\n",
    "### if the regrid on the dirs is already done\n",
    "regridded = True\n",
    "\n",
    "## Passirio basin\n",
    "basin = 'passirio'\n",
    "lat = ( 46.68, 46.945 )\n",
    "lon = ( 11.015, 11.38 )\n",
    "\n",
    "# ## Plan basin\n",
    "# basin = 'plan'\n",
    "# lat = ( 46.7145853, 46.8251415 )\n",
    "# lon = ( 11.0198472, 11.117037 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = glob.glob( data_path + '*/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Include standard modules\n",
    "# import argparse\n",
    "\n",
    "# # Initiate the parser\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# # Add long and short argument\n",
    "# parser.add_argument(\"--path\", \"-p\", help=\"set input path\")\n",
    "# parser.add_argument(\"--variable\", \"-var\", help=\"set variable to process\")\n",
    "# parser.add_argument(\"--leadtime\", \"-l\", help=\"set starting lead time\")\n",
    "\n",
    "# # Read arguments from the command line\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# # Check for --path\n",
    "# if args.path:\n",
    "#     print(\"Set path to %s\" % args.path)\n",
    "#     dirs = [args.path]\n",
    "# # Check for --path\n",
    "# if args.variable:\n",
    "#     print(\"Set variable to %s\" % args.variable)\n",
    "#     variables = [args.variable]\n",
    "# # Check for --path\n",
    "# if args.leadtime:\n",
    "#     print(\"Set leadtime to %s\" % args.leadtime)\n",
    "#     init_lead_time = int(args.leadtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_time_array = []\n",
    "for n in range(1, lead_hours+1):\n",
    "    n = str(n).zfill(3)\n",
    "    lead_time_array.append(n)\n",
    "\n",
    "for el in dirs:\n",
    "\n",
    "    # print(el)\n",
    "    \n",
    "    tmp = el.split('/')\n",
    "    current_date = str(tmp[len(tmp) - 2])\n",
    "\n",
    "    print(\"Current date: \" + current_date)\n",
    "\n",
    "    for variable in variables:\n",
    "\n",
    "        print(\"Variable: \" + variable)\n",
    "\n",
    "        for n in range(init_lead_time, lead_hours+1):\n",
    "\n",
    "            # data_df = None\n",
    "\n",
    "            lead_mins = str( 60*n )\n",
    "\n",
    "            n = str(n).zfill(3)\n",
    "            print(\"Lead hour: \" + n)\n",
    "\n",
    "            original_file_to_read = el + 'icon-d2*' + current_date + \\\n",
    "                init_ref + '_' + n + '*' + variable + '*.grib2'\n",
    "            print(\"Original file to regrid: \" + original_file_to_read)\n",
    "\n",
    "            # create inner dirs structure\n",
    "            current_file_path = el + n + '/' + basin + '/' + variable + '/'\n",
    "            mkNestedDir(current_file_path)\n",
    "            os.chdir(current_file_path)\n",
    "\n",
    "            original_current_file = glob.glob(original_file_to_read)\n",
    "\n",
    "            if len(original_current_file) == 0:\n",
    "\n",
    "                copy_process = \"cp {} {}\".format(\n",
    "                    dummy_output, current_file_path + os.path.basename(current_file))\n",
    "                subprocess.run(copy_process, shell=True,\n",
    "                               check=True, executable='/bin/bash')\n",
    "\n",
    "            else:\n",
    "\n",
    "                # identify and move current_file into current_file_path\n",
    "                file_to_read = current_file_path + '*regridded*' + current_date + \\\n",
    "                    init_ref + '_' + n + '*' + variable + '*.grib2'\n",
    "                print(\"Check INNER file regridded: \" + file_to_read)\n",
    "\n",
    "                current_file = glob.glob(file_to_read)\n",
    "\n",
    "                if len(current_file) == 0:\n",
    "\n",
    "                    transform( original_current_file[0], current_file_path)\n",
    "                    regrid_dir( current_file_path )\n",
    "                    current_file = glob.glob(file_to_read)\n",
    "\n",
    "                    current_file = current_file[0]\n",
    "\n",
    "                else:\n",
    "                    current_file = current_file[0]\n",
    "                    print(\"Current file: \" + current_file)\n",
    "                \n",
    "                os.chdir( current_file_path )\n",
    "                # extract data to output.csv\n",
    "                extraction_process = '''docker run --rm --volume $(pwd):/local \\\n",
    "                    deutscherwetterdienst/python-eccodes grib_get_data -p date,time,stepRange,shortName {} > output.csv'''\n",
    "                extraction_process = extraction_process.format(\n",
    "                    os.path.basename(current_file))\n",
    "                subprocess.run(extraction_process, shell=True,\n",
    "                               check=True, executable='/bin/bash')\n",
    "\n",
    "            # extract data to process\n",
    "            # read exported data and cut to the ROI\n",
    "            data_df = dd.read_csv(current_file_path + 'output.csv', sep='\\s+', header=None, skiprows=1,\n",
    "                                  names=['lat', 'lon', 'values', 'date', 'time', 'step_range', 'name'], comment=\"L\")\n",
    "\n",
    "            data_df = data_df.astype({'lat': float, 'lon': float, 'values': float,\n",
    "                                      'date': str, 'time': str, 'step_range': str, 'name': str})\n",
    "\n",
    "            data_df = data_df[data_df['lat'] >= lat[0]]\n",
    "            data_df = data_df[data_df['lat'] <= lat[1]]\n",
    "            data_df = data_df[data_df['lon'] >= lon[0]]\n",
    "            data_df = data_df[data_df['lon'] <= lon[1]]\n",
    "\n",
    "            if variable == \"temperature\":\n",
    "                data_df = data_df[data_df['step_range'] == lead_mins]\n",
    "            elif variable == \"precipitation\":\n",
    "                data_df = data_df[data_df['step_range'] == '0-' + lead_mins]\n",
    "\n",
    "            data_df = data_df.compute()\n",
    "\n",
    "            # export data to the new structure\n",
    "            interruptor = int(len(data_df) / ensemble_number)\n",
    "\n",
    "            metadata = True\n",
    "            for ens in range(ensemble_number):\n",
    "\n",
    "                print(\"Ensemble #:\" + str(ens+1))\n",
    "\n",
    "                # create ensamble directory\n",
    "                m = str(ens+1).zfill(3)\n",
    "                ens_file_path = current_file_path + m + '/'\n",
    "                mkNestedDir(ens_file_path)\n",
    "\n",
    "                current_data = pd.DataFrame(\n",
    "                    columns=['ID', 'lat', 'lon', 'values'])\n",
    "                ids = []\n",
    "                lats = []\n",
    "                lons = []\n",
    "                vals = []\n",
    "\n",
    "                for i in range(interruptor*ens, interruptor*(1+ens)):\n",
    "\n",
    "                    point_id = i + 1 - interruptor*ens\n",
    "\n",
    "                    ids.append(point_id)\n",
    "                    lats.append(data_df.iloc[i]['lat'])\n",
    "                    lons.append(data_df.iloc[i]['lon'])\n",
    "                    vals.append(data_df.iloc[i]['values'])\n",
    "\n",
    "                current_data['ID'] = ids\n",
    "                current_data['lat'] = lats\n",
    "                current_data['lon'] = lons\n",
    "                current_data['values'] = vals\n",
    "\n",
    "                current_data.to_csv(ens_file_path + 'output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
